{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Fine-Tuning for Job Description Classification with SageMaker & MLflow\n",
    "\n",
    "This notebook orchestrates a SageMaker Pipeline to fine-tune Llama 3 for job description classification. \n",
    "It uses pre-generated raw data from S3 (created by `scripts/python/generate_and_upload_raw_data.py`), \n",
    "a dedicated training script (`scripts/python/finetune_entrypoint.py`), and MLflow for experiment tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "**Important:** If you encounter `OSError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/fsspec-XYZ.dist-info/METADATA'`, run the following in a new cell first, then **RESTART THE KERNEL** and run this cell again:\n",
    "```python\n",
    "# !pip install --ignore-installed --no-deps --no-cache-dir fsspec==2023.6.0 # Or the problematic version\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05af8651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting fsspec==2023.6.0\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec\n",
      "Successfully installed fsspec-2023.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --ignore-installed --no-deps --no-cache-dir fsspec==2023.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "442307c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker==2.225.0  datasets transformers mlflow==2.13.2 sagemaker-mlflow==0.1.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"sagemaker>=2.200.0\" \"datasets>=2.18.0\" \"transformers>=4.38.0,<4.41.0\" \"mlflow>=2.9.0\" \"sagemaker-mlflow>=0.1.0\" --quiet\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --quiet # Ensure cuXXX matches your training instance GPU CUDA version\n",
    "# !pip install \"accelerate>=0.28.0\" \"bitsandbytes>=0.41.0,<0.43.0\" \"scikit-learn>=1.0.0\" \"pandas\" \"matplotlib\" \"seaborn\" \"huggingface_hub\" \"s3fs\" \"peft>=0.9.0,<0.12.0\" --quiet # Pin peft for transformers 4.36-4.40 compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Adding project root to sys.path: /home/sagemaker-user\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.7.0+cu118 available.\n",
      "INFO:datasets:TensorFlow version 2.12.1 available.\n",
      "INFO:datasets:JAX version 0.4.20 available.\n",
      "2025-06-04 22:25:00.750720: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "import json\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.function_step import step\n",
    "from sagemaker.workflow.parameters import ParameterString, ParameterInteger, ParameterFloat\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "\n",
    "# --- Add project root to sys.path for local module imports ---\n",
    "import sys\n",
    "notebook_dir = os.getcwd() \n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if project_root not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root}\")\n",
    "    sys.path.insert(0, project_root)\n",
    "else:\n",
    "    print(f\"Project root already in sys.path: {project_root}\")\n",
    "# --- End Path Setup ---\n",
    "\n",
    "# Import functions from your scripts\n",
    "from steps.preprocess_job_descriptions import preprocess_data\n",
    "from steps.finetune_llama3_classifier import launch_hf_training_job # This is your LAUNCHER script's main function\n",
    "from steps.evaluation_classifier import evaluate_model\n",
    "\n",
    "# os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SageMaker Session and IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "413c63df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# print(os.environ.get(\"AWS_ROLE_ARN\"))\n",
    "\n",
    "# import boto3\n",
    "# try:\n",
    "#     client = boto3.client('sts')\n",
    "#     identity = client.get_caller_identity()\n",
    "#     print(identity)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error getting caller identity: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a3a7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create session with explicit boto session to avoid config issues\n",
    "boto_session = boto3.Session(region_name='us-east-1')\n",
    "sess = sagemaker.Session(boto_session=boto_session)\n",
    "\n",
    "region = sess.boto_region_name\n",
    "default_bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Execution Role: arn:aws:iam::174671970284:role/service-role/AmazonSageMaker-ExecutionRole-20240216T153805\n",
      "SageMaker Session region: us-east-1, bucket: sagemaker-us-east-1-174671970284\n",
      "Region: us-east-1\n",
      "Default bucket: sagemaker-us-east-1-174671970284\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    print(f\"SageMaker Execution Role: {role}\")\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    print(\"Could not automatically get SageMaker execution role. Please ensure it's configured or specify manually.\")\n",
    "    raise ValueError(\"SageMaker execution role not found.\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "default_bucket = sess.default_bucket()\n",
    "print(f\"SageMaker Session region: {region}, bucket: {default_bucket}\")\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Default bucket: {default_bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = \"JobDescClassification-Llama3-Pipeline-V5\" \n",
    "base_job_prefix = \"job-desc-classify\" \n",
    "\n",
    "mlflow_tracking_server_arn = \"arn:aws:sagemaker:us-east-1:174671970284:mlflow-tracking-server/mlflow-d-8mkvrvo3fobb-27-10-47-37\" # <--- REPLACE THIS\n",
    "mlflow_experiment_name = \"JobDescriptionClassification-Llama3-FineTuning\"\n",
    "\n",
    "model_id_default = \"meta-llama/Meta-Llama-3-8B\" \n",
    "\n",
    "processed_data_s3_prefix = f\"{base_job_prefix}/processed_data/v3\" \n",
    "\n",
    "# default_raw_data_s3_uri = f\"s3://{default_bucket}/raw_job_data/poc_multilingual_01/raw_jds_translated.jsonl\"\n",
    "default_raw_data_s3_uri = \"s3://sagemaker-us-east-1-174671970284/raw_job_data/poc_multilingual_set_20250604_214156/raw_jds_translated_v2.jsonl\"\n",
    "\n",
    "preprocess_instance_type = \"ml.m5.large\"\n",
    "finetune_launcher_instance_type = \"ml.m5.large\" \n",
    "default_training_instance_type = \"ml.g5.12xlarge\" \n",
    "evaluation_instance_type = \"ml.g5.2xlarge\" \n",
    "\n",
    "default_hf_training_image_uri = f'763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-training:2.1.0-transformers4.36.0-gpu-py310-cu121-ubuntu20.04'\n",
    "\n",
    "if \"your-region\" in mlflow_tracking_server_arn:\n",
    "    print(\"ERROR: MLflow Tracking Server ARN is a placeholder. Please replace it.\")\n",
    "if \"raw_job_data/poc_multilingual_01/raw_jds_translated.jsonl\" in default_raw_data_s3_uri:\n",
    "    # This is a very basic check and might not be accurate for all S3 path styles\n",
    "    print(f\"WARNING: Default RawDatasetS3URI is set to '{default_raw_data_s3_uri}'. Ensure this S3 URI points to your generated raw dataset or override this parameter when starting the pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_raw_data_s3_uri = ParameterString(name=\"RawDatasetS3URI\", default_value=default_raw_data_s3_uri)\n",
    "param_job_desc_column = ParameterString(name=\"JobDescriptionColumn\", default_value=\"job_description_text\")\n",
    "param_category_column = ParameterString(name=\"CategoryColumn\", default_value=\"category_label\")\n",
    "param_test_split_fraction = ParameterFloat(name=\"TestSplitFraction\", default_value=0.15)\n",
    "param_validation_split_fraction = ParameterFloat(name=\"ValidationSplitFraction\", default_value=0.15)\n",
    "param_max_samples_per_split = ParameterInteger(name=\"MaxSamplesPerSplit\", default_value=-1)\n",
    "\n",
    "param_model_id = ParameterString(name=\"ModelIdentifier\", default_value=model_id_default)\n",
    "param_training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=default_training_instance_type)\n",
    "param_training_instance_count = ParameterInteger(name=\"TrainingInstanceCount\", default_value=1)\n",
    "param_hf_training_image_uri = ParameterString(name=\"HFTrainingImageURI\", default_value=default_hf_training_image_uri)\n",
    "\n",
    "param_finetune_epochs = ParameterInteger(name=\"FineTuneEpochs\", default_value=1)\n",
    "param_per_device_train_batch_size = ParameterInteger(name=\"PerDeviceTrainBatchSize\", default_value=1)\n",
    "param_learning_rate = ParameterFloat(name=\"LearningRate\", default_value=0.0002)\n",
    "param_lora_r = ParameterInteger(name=\"LoraR\", default_value=8)\n",
    "param_lora_alpha = ParameterInteger(name=\"LoraAlpha\", default_value=16)\n",
    "param_lora_dropout = ParameterFloat(name=\"LoraDropout\", default_value=0.05)\n",
    "param_lora_target_modules = ParameterString(name=\"LoraTargetModules\", default_value=\"q_proj,v_proj,k_proj,o_proj\")\n",
    "param_merge_weights = ParameterString(name=\"MergeWeights\", default_value=\"True\")\n",
    "param_hf_token = ParameterString(name=\"HuggingFaceToken\", default_value=\"OPTIONAL_HF_TOKEN_PLACEHOLDER\")\n",
    "\n",
    "param_eval_batch_size = ParameterInteger(name=\"EvaluationBatchSize\", default_value=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Pipeline Steps using `@step` decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A. Preprocessing Step\n",
    "# @step(\n",
    "#     name=\"PreprocessJobData\",\n",
    "#     instance_type=preprocess_instance_type,\n",
    "#     keep_alive_period_in_seconds=300\n",
    "# )\n",
    "# def sm_pipeline_preprocess_data_step(\n",
    "#     raw_s3_uri: str,\n",
    "#     s3_bucket: str,\n",
    "#     s3_prefix: str,\n",
    "#     jd_column: str,\n",
    "#     cat_column: str,\n",
    "#     test_frac_val: float,\n",
    "#     val_frac_val: float,\n",
    "#     max_samples_val: int,\n",
    "#     mlflow_arn_val: str,\n",
    "#     mlflow_exp_val: str,\n",
    "#     exec_id: str,\n",
    "# ):\n",
    "#     actual_max_samples = None if max_samples_val < 0 else max_samples_val\n",
    "#     return preprocess_data(\n",
    "#         raw_dataset_identifier=raw_s3_uri,\n",
    "#         s3_output_bucket=s3_bucket,\n",
    "#         s3_output_prefix=s3_prefix,\n",
    "#         job_desc_column=jd_column,\n",
    "#         category_column=cat_column,\n",
    "#         test_split_fraction=test_frac_val,\n",
    "#         validation_from_train_fraction=val_frac_val,\n",
    "#         max_samples_per_split=actual_max_samples,\n",
    "#         mlflow_arn=mlflow_arn_val,\n",
    "#         experiment_name=mlflow_exp_val,\n",
    "#         run_name=exec_id,\n",
    "#     )\n",
    "\n",
    "# # B. Fine-tuning Launcher Step\n",
    "# @step(\n",
    "#     name=\"LaunchHFFineTuning\",\n",
    "#     instance_type=finetune_launcher_instance_type, \n",
    "#     keep_alive_period_in_seconds=300 \n",
    "# )\n",
    "# def sm_pipeline_finetune_launcher_step(\n",
    "#     processed_data_info_dict: dict,\n",
    "#     sagemaker_iam_role: str,\n",
    "#     train_instance_type_str: str,\n",
    "#     train_instance_count_int: int,\n",
    "#     hf_image_uri_str: str,\n",
    "#     model_id_str: str,\n",
    "#     epochs_int: int,\n",
    "#     batch_size_int: int,\n",
    "#     lr_float: float,\n",
    "#     lora_r_int: int,\n",
    "#     lora_alpha_int: int,\n",
    "#     lora_dropout_float: float,\n",
    "#     lora_targets_str: str,\n",
    "#     merge_weights_str: str,\n",
    "#     hf_token_str: str,\n",
    "#     mlflow_arn_str: str,\n",
    "#     mlflow_exp_str: str,\n",
    "#     pipeline_exec_id_str: str,\n",
    "# ):\n",
    "#     merge_weights_bool = merge_weights_str.lower() == 'true'\n",
    "#     actual_hf_token = hf_token_str if hf_token_str and hf_token_str != \"OPTIONAL_HF_TOKEN_PLACEHOLDER\" else None\n",
    "    \n",
    "#     # launch_hf_training_job is from steps.finetune_llama3_classifier (your launcher script)\n",
    "#     # Ensure this launcher script internally sets source_dir=\"scripts\" and entry_point=\"python/finetune_entrypoint.py\"\n",
    "#     return launch_hf_training_job(\n",
    "#         role=sagemaker_iam_role,\n",
    "#         image_uri=hf_image_uri_str,\n",
    "#         instance_type=train_instance_type_str,\n",
    "#         instance_count=train_instance_count_int,\n",
    "#         train_s3_uri=processed_data_info_dict['train'],\n",
    "#         validation_s3_uri=processed_data_info_dict['validation'],\n",
    "#         # The following two are now expected to be hardcoded/managed within your launcher script:\n",
    "#         entry_point_script=\"python/finetune_entrypoint.py\", # This should match what your launcher expects or uses internally\n",
    "#         source_directory=\"scripts\", # This should match what your launcher expects or uses internally\n",
    "#         model_id_hf=model_id_str,\n",
    "#         epochs_val=epochs_int,\n",
    "#         per_device_train_batch_size_val=batch_size_int,\n",
    "#         learning_rate_val=lr_float,\n",
    "#         lora_r_val=lora_r_int,\n",
    "#         lora_alpha_val=lora_alpha_int,\n",
    "#         lora_dropout_val=lora_dropout_float,\n",
    "#         lora_target_modules_val=lora_targets_str,\n",
    "#         merge_weights_val=merge_weights_bool,\n",
    "#         hf_token_val=actual_hf_token,\n",
    "#         mlflow_tracking_arn=mlflow_arn_str,\n",
    "#         mlflow_experiment=mlflow_exp_str,\n",
    "#         pipeline_run_id=pipeline_exec_id_str,\n",
    "#         base_job_name_prefix=f\"job-clf-{model_id_str.split('/')[-1].replace('_','-')}\"\n",
    "#     )\n",
    "\n",
    "# # C. Evaluation Step\n",
    "# @step(\n",
    "#     name=\"EvaluateClassifier\",\n",
    "#     instance_type=evaluation_instance_type,\n",
    "#     image_uri=default_hf_training_image_uri, \n",
    "#     keep_alive_period_in_seconds=600\n",
    "# )\n",
    "# def sm_pipeline_evaluate_model_step(\n",
    "#     finetune_launcher_output_dict: dict, \n",
    "#     processed_data_info_dict: dict, \n",
    "#     eval_batch_size_int: int, \n",
    "#     mlflow_arn_str: str,\n",
    "#     mlflow_exp_str: str,\n",
    "#     pipeline_exec_id_str: str \n",
    "# ):\n",
    "#     mlflow_model_uri = f\"runs:/{pipeline_exec_id_str}/fine_tuned_classifier_model\" \n",
    "    \n",
    "#     return evaluate_model(\n",
    "#         model_s3_path_or_mlflow_uri=mlflow_model_uri, \n",
    "#         test_data_s3_path=processed_data_info_dict['test'],\n",
    "#         poc_categories_s3_path=processed_data_info_dict['categories_s3_path'],\n",
    "#         batch_size=eval_batch_size_int,\n",
    "#         mlflow_arn=mlflow_arn_str,\n",
    "#         experiment_name=mlflow_exp_str,\n",
    "#         run_id=pipeline_exec_id_str \n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c165b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A. Preprocessing Step\n",
    "# The 'preprocess_data' function itself will be executed as the step's logic.\n",
    "# Arguments passed here must match the signature of 'preprocess_data'.\n",
    "step_preprocess_obj = step( \n",
    "    preprocess_data, # Directly pass the imported function\n",
    "    name=\"PreprocessJobData\", # Step name in the pipeline\n",
    "    instance_type=preprocess_instance_type,\n",
    "    keep_alive_period_in_seconds=300\n",
    ")\n",
    "\n",
    "# B. Fine-tuning Launcher Step\n",
    "# The 'launch_hf_training_job' function itself will be executed.\n",
    "step_finetune_launcher_obj = step(\n",
    "    launch_hf_training_job, # Directly pass the imported launcher function\n",
    "    name=\"LaunchHFFineTuning\",\n",
    "    instance_type=finetune_launcher_instance_type, # Instance for the launcher step itself\n",
    "    keep_alive_period_in_seconds=300 \n",
    "    # If launch_hf_training_job needs many dependencies not in the default @step image,\n",
    "    # you might need to specify an image_uri here for the launcher step too,\n",
    "    # or ensure its dependencies are minimal (like just sagemaker, boto3).\n",
    ")\n",
    "\n",
    "# C. Evaluation Step\n",
    "# The 'evaluate_model' function itself will be executed.\n",
    "step_evaluate_obj = step(\n",
    "    evaluate_model, # Directly pass the imported evaluation function\n",
    "    name=\"EvaluateClassifier\",\n",
    "    instance_type=evaluation_instance_type,\n",
    "    image_uri=default_hf_training_image_uri, # Reuse training image if it has necessary eval libs\n",
    "    keep_alive_period_in_seconds=600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Construct the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "308c2ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the Preprocessing Step Object\n",
    "preprocess_step_output_data = step_preprocess_obj( # Use the step object from above\n",
    "    raw_dataset_identifier=param_raw_data_s3_uri, # Argument for preprocess_data\n",
    "    s3_output_bucket=default_bucket,              # Argument for preprocess_data\n",
    "    s3_output_prefix=processed_data_s3_prefix,    # Argument for preprocess_data\n",
    "    job_desc_column=param_job_desc_column,        # Argument for preprocess_data\n",
    "    category_column=param_category_column,        # Argument for preprocess_data\n",
    "    test_split_fraction=param_test_split_fraction, # Argument for preprocess_data\n",
    "    validation_from_train_fraction=param_validation_split_fraction, # Argument for preprocess_data\n",
    "    max_samples_per_split=param_max_samples_per_split,             # Argument for preprocess_data\n",
    "    mlflow_arn=mlflow_tracking_server_arn,        # Argument for preprocess_data\n",
    "    experiment_name=mlflow_experiment_name,       # Argument for preprocess_data\n",
    "    run_name=ExecutionVariables.PIPELINE_EXECUTION_ID # Argument for preprocess_data\n",
    ")\n",
    "\n",
    "# Call the Fine-tuning Launcher Step Object\n",
    "finetune_step_output_data = step_finetune_launcher_obj( # Use the step object\n",
    "    # Arguments for launch_hf_training_job:\n",
    "    role=role, \n",
    "    image_uri=param_hf_training_image_uri, # Actual image for the training job\n",
    "    instance_type=param_training_instance_type, # Actual instance for the training job\n",
    "    instance_count=param_training_instance_count,\n",
    "    train_s3_uri=preprocess_step_output_data['train'], # Output from preprocess_data\n",
    "    validation_s3_uri=preprocess_step_output_data['validation'], # Output from preprocess_data\n",
    "    # entry_point_script and source_directory are now hardcoded inside launch_hf_training_job\n",
    "    train_file=\"train_dataset.jsonl\",                  # Default or make ParameterString\n",
    "    validation_file=\"validation_dataset.jsonl\",        # Default or make ParameterString\n",
    "    entry_point_script=\"python/finetune_entrypoint.py\",# Relative to source_directory\n",
    "    source_directory=\"scripts\",                        # Directory containing entrypoint & requirements.txt\n",
    "    base_job_name_prefix=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "    model_id_hf=param_model_id, \n",
    "    epochs_val=param_finetune_epochs,\n",
    "    per_device_train_batch_size_val=param_per_device_train_batch_size,\n",
    "    learning_rate_val=param_learning_rate,\n",
    "    lora_r_val=param_lora_r,\n",
    "    lora_alpha_val=param_lora_alpha,\n",
    "    lora_dropout_val=param_lora_dropout,\n",
    "    lora_target_modules_val=param_lora_target_modules,\n",
    "    merge_weights_val=(param_merge_weights == 'True'), # Convert ParameterString to bool\n",
    "    hf_token_val=param_hf_token, # Pass the ParameterString, launcher handles placeholder\n",
    "    mlflow_tracking_arn=mlflow_tracking_server_arn,\n",
    "    mlflow_experiment=mlflow_experiment_name,\n",
    "    pipeline_run_id=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "    # base_job_name_prefix=f\"job-clf-{param_model_id.replace('/','-').replace('_','-')}\" # Example prefix for training job\n",
    ")\n",
    "\n",
    "# Call the Evaluation Step Object\n",
    "evaluate_step_output_data = step_evaluate_obj( # Use the step object\n",
    "    # Arguments for evaluate_model:\n",
    "    # Construct MLflow URI using pipeline execution ID (which was used as run_id in finetune_entrypoint)\n",
    "    model_s3_path_or_mlflow_uri=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "    test_data_s3_path=preprocess_step_output_data['test'],\n",
    "    poc_categories_s3_path=preprocess_step_output_data['categories_s3_path'],\n",
    "    batch_size=param_eval_batch_size,\n",
    "    mlflow_arn=mlflow_tracking_server_arn,\n",
    "    experiment_name=mlflow_experiment_name,\n",
    "    run_id=ExecutionVariables.PIPELINE_EXECUTION_ID # eval logs under same parent run\n",
    ")\n",
    "\n",
    "# Create the pipeline (this part remains the same)\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        param_raw_data_s3_uri,\n",
    "        param_job_desc_column,\n",
    "        param_category_column,\n",
    "        param_test_split_fraction,\n",
    "        param_validation_split_fraction,\n",
    "        param_max_samples_per_split,\n",
    "        param_model_id,\n",
    "        param_training_instance_type,\n",
    "        param_training_instance_count,\n",
    "        param_hf_training_image_uri,\n",
    "        param_finetune_epochs,\n",
    "        param_per_device_train_batch_size,\n",
    "        param_learning_rate,\n",
    "        param_lora_r,\n",
    "        param_lora_alpha,\n",
    "        param_lora_dropout,\n",
    "        param_lora_target_modules,\n",
    "        param_merge_weights,\n",
    "        param_hf_token,\n",
    "        param_eval_batch_size\n",
    "    ],\n",
    "    steps=[evaluate_step_output_data], # Last step in the chain\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_step_output_data = sm_pipeline_preprocess_data_step(\n",
    "#     raw_s3_uri=param_raw_data_s3_uri,\n",
    "#     s3_bucket=default_bucket, \n",
    "#     s3_prefix=processed_data_s3_prefix,\n",
    "#     jd_column=param_job_desc_column,\n",
    "#     cat_column=param_category_column,\n",
    "#     test_frac_val=param_test_split_fraction,\n",
    "#     val_frac_val=param_validation_split_fraction,\n",
    "#     max_samples_val=param_max_samples_per_split, \n",
    "#     mlflow_arn_val=mlflow_tracking_server_arn, \n",
    "#     mlflow_exp_val=mlflow_experiment_name, \n",
    "#     exec_id=ExecutionVariables.PIPELINE_EXECUTION_ID\n",
    "# )\n",
    "\n",
    "# finetune_step_output_data = sm_pipeline_finetune_launcher_step(\n",
    "#     processed_data_info_dict=preprocess_step_output_data, \n",
    "#     sagemaker_iam_role=role, \n",
    "#     train_instance_type_str=param_training_instance_type,\n",
    "#     train_instance_count_int=param_training_instance_count,\n",
    "#     hf_image_uri_str=param_hf_training_image_uri,\n",
    "#     model_id_str=param_model_id, \n",
    "#     epochs_int=param_finetune_epochs,\n",
    "#     batch_size_int=param_per_device_train_batch_size,\n",
    "#     lr_float=param_learning_rate,\n",
    "#     lora_r_int=param_lora_r,\n",
    "#     lora_alpha_int=param_lora_alpha,\n",
    "#     lora_dropout_float=param_lora_dropout,\n",
    "#     lora_targets_str=param_lora_target_modules,\n",
    "#     merge_weights_str=param_merge_weights,\n",
    "#     hf_token_str=param_hf_token,\n",
    "#     mlflow_arn_str=mlflow_tracking_server_arn,\n",
    "#     mlflow_exp_str=mlflow_experiment_name,\n",
    "#     pipeline_exec_id_str=ExecutionVariables.PIPELINE_EXECUTION_ID\n",
    "# )\n",
    "\n",
    "# evaluate_step_output_data = sm_pipeline_evaluate_model_step(\n",
    "#     finetune_launcher_output_dict=finetune_step_output_data, \n",
    "#     processed_data_info_dict=preprocess_step_output_data, \n",
    "#     eval_batch_size_int=param_eval_batch_size,\n",
    "#     mlflow_arn_str=mlflow_tracking_server_arn,\n",
    "#     mlflow_exp_str=mlflow_experiment_name,\n",
    "#     pipeline_exec_id_str=ExecutionVariables.PIPELINE_EXECUTION_ID\n",
    "# )\n",
    "\n",
    "# pipeline = Pipeline(\n",
    "#     name=pipeline_name,\n",
    "#     parameters=[\n",
    "#         param_raw_data_s3_uri,\n",
    "#         param_job_desc_column,\n",
    "#         param_category_column,\n",
    "#         param_test_split_fraction,\n",
    "#         param_validation_split_fraction,\n",
    "#         param_max_samples_per_split,\n",
    "#         param_model_id,\n",
    "#         param_training_instance_type,\n",
    "#         param_training_instance_count,\n",
    "#         param_hf_training_image_uri,\n",
    "#         param_finetune_epochs,\n",
    "#         param_per_device_train_batch_size,\n",
    "#         param_learning_rate,\n",
    "#         param_lora_r,\n",
    "#         param_lora_alpha,\n",
    "#         param_lora_dropout,\n",
    "#         param_lora_target_modules,\n",
    "#         param_merge_weights,\n",
    "#         param_hf_token,\n",
    "#         param_eval_batch_size\n",
    "#     ],\n",
    "#     steps=[evaluate_step_output_data],\n",
    "#     sagemaker_session=sess\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Upsert and Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 22:39:54,612 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-174671970284/JobDescClassification-Llama3-Pipeline-V5/EvaluateClassifier/2025-06-04-22-39-54-611/function\n",
      "2025-06-04 22:39:54,714 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-174671970284/JobDescClassification-Llama3-Pipeline-V5/EvaluateClassifier/2025-06-04-22-39-54-611/arguments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Upserting the pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-06-04 22:39:54,866 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-174671970284/JobDescClassification-Llama3-Pipeline-V5/PreprocessJobData/2025-06-04-22-39-54-611/function\n",
      "2025-06-04 22:39:54,999 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-174671970284/JobDescClassification-Llama3-Pipeline-V5/PreprocessJobData/2025-06-04-22-39-54-611/arguments\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-06-04 22:39:55,586 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-174671970284/JobDescClassification-Llama3-Pipeline-V5/EvaluateClassifier/2025-06-04-22-39-55-585/function\n",
      "2025-06-04 22:39:55,644 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-174671970284/JobDescClassification-Llama3-Pipeline-V5/EvaluateClassifier/2025-06-04-22-39-55-585/arguments\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-06-04 22:39:55,822 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-174671970284/JobDescClassification-Llama3-Pipeline-V5/PreprocessJobData/2025-06-04-22-39-55-585/function\n",
      "2025-06-04 22:39:55,910 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-174671970284/JobDescClassification-Llama3-Pipeline-V5/PreprocessJobData/2025-06-04-22-39-55-585/arguments\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 'JobDescClassification-Llama3-Pipeline-V5' upserted successfully.\n",
      "\n",
      "Starting pipeline execution...\n",
      "Pipeline execution started with ARN: arn:aws:sagemaker:us-east-1:174671970284:pipeline/JobDescClassification-Llama3-Pipeline-V5/execution/h7okbf6qbqn9\n"
     ]
    }
   ],
   "source": [
    "if \"your-region\" in mlflow_tracking_server_arn:\n",
    "    print(\"ERROR: MLflow Tracking Server ARN is a placeholder. Update it in cell [3].\")\n",
    "else:\n",
    "    print(\"\\nUpserting the pipeline...\")\n",
    "    try:\n",
    "        pipeline.upsert(role_arn=role)\n",
    "        print(f\"Pipeline '{pipeline_name}' upserted successfully.\")\n",
    "\n",
    "        print(\"\\nStarting pipeline execution...\")\n",
    "        execution = pipeline.start(\n",
    "            parameters={}\n",
    "        )\n",
    "        print(f\"Pipeline execution started with ARN: {execution.arn}\")\n",
    "        execution.describe()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during pipeline upsert or start: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Clean up (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To delete the pipeline definition from SageMaker:\n",
    "# try:\n",
    "#     pipeline.delete()\n",
    "#     print(f\"Pipeline '{pipeline_name}' deleted.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error deleting pipeline '{pipeline_name}': {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "sagemaker": {
   "image_uri": "arn:aws:sagemaker:us-east-1:000000000000:image/sagemaker-data-science-310-v1",
   "instance_type": "ml.t3.medium"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

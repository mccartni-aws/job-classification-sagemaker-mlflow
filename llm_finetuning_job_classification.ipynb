{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Fine-Tuning for Job Description Classification with SageMaker & MLflow\n",
    "\n",
    "This notebook orchestrates a SageMaker Pipeline to fine-tune Llama 3 for job description classification. \n",
    "It uses pre-generated raw data from S3 (created by `scripts/python/generate_and_upload_raw_data.py`), \n",
    "a dedicated training script (`scripts/python/finetune_entrypoint.py`), and MLflow for experiment tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "**Important:** If you encounter `OSError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/fsspec-XYZ.dist-info/METADATA'`, run the following in a new cell first, then **RESTART THE KERNEL** and run this cell again:\n",
    "```python\n",
    "# !pip install --ignore-installed --no-deps --no-cache-dir fsspec==2023.6.0 # Or the problematic version\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05af8651",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --ignore-installed --no-deps --no-cache-dir fsspec==2023.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "442307c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker  datasets transformers mlflow sagemaker-mlflow --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"sagemaker>=2.200.0\" \"datasets>=2.18.0\" \"transformers>=4.38.0,<4.41.0\" \"mlflow>=2.9.0\" \"sagemaker-mlflow>=0.1.0\" --quiet\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --quiet # Ensure cuXXX matches your training instance GPU CUDA version\n",
    "# !pip install \"accelerate>=0.28.0\" \"bitsandbytes>=0.41.0,<0.43.0\" \"scikit-learn>=1.0.0\" \"pandas\" \"matplotlib\" \"seaborn\" \"huggingface_hub\" \"s3fs\" \"peft>=0.9.0,<0.12.0\" --quiet # Pin peft for transformers 4.36-4.40 compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Project root already in sys.path: /home/sagemaker-user\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:57:35.573356: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "import json\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.function_step import step\n",
    "from sagemaker.workflow.parameters import ParameterString, ParameterInteger, ParameterFloat\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "\n",
    "# --- Add project root to sys.path for local module imports ---\n",
    "import sys\n",
    "notebook_dir = os.getcwd() \n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if project_root not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root}\")\n",
    "    sys.path.insert(0, project_root)\n",
    "else:\n",
    "    print(f\"Project root already in sys.path: {project_root}\")\n",
    "# --- End Path Setup ---\n",
    "\n",
    "# Import functions from your scripts\n",
    "from steps.preprocess_job_descriptions import preprocess_data\n",
    "from steps.finetune_llama3_classifier import launch_hf_training_job # This is your LAUNCHER script's main function\n",
    "from steps.evaluation_classifier import evaluate_model\n",
    "\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SageMaker Session and IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "413c63df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'UserId': 'AROASRK2CX7WPM2ML6UZA:SageMaker', 'Account': '174671970284', 'Arn': 'arn:aws:sts::174671970284:assumed-role/AmazonSageMaker-ExecutionRole-20240216T153805/SageMaker', 'ResponseMetadata': {'RequestId': 'deb030f4-3e00-42c0-90c7-3bc16f70f630', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'deb030f4-3e00-42c0-90c7-3bc16f70f630', 'content-type': 'text/xml', 'content-length': '470', 'date': 'Thu, 29 May 2025 22:00:48 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get(\"AWS_ROLE_ARN\"))\n",
    "\n",
    "import boto3\n",
    "try:\n",
    "    client = boto3.client('sts')\n",
    "    identity = client.get_caller_identity()\n",
    "    print(identity)\n",
    "except Exception as e:\n",
    "    print(f\"Error getting caller identity: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to load the config file from the location: /home/sagemaker-user/job-classification-sagemaker-mlflowProvide a valid file path",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/sagemaker-user/job-classification-sagemaker-mlflow/llm_finetuning_job_classification.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://gom8fjg3ypjgkmk.studio.us-east-1.sagemaker.aws/home/sagemaker-user/job-classification-sagemaker-mlflow/llm_finetuning_job_classification.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# try:\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://gom8fjg3ypjgkmk.studio.us-east-1.sagemaker.aws/home/sagemaker-user/job-classification-sagemaker-mlflow/llm_finetuning_job_classification.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#     role = sagemaker.get_execution_role()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://gom8fjg3ypjgkmk.studio.us-east-1.sagemaker.aws/home/sagemaker-user/job-classification-sagemaker-mlflow/llm_finetuning_job_classification.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#     print(f\"SageMaker Execution Role: {role}\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://gom8fjg3ypjgkmk.studio.us-east-1.sagemaker.aws/home/sagemaker-user/job-classification-sagemaker-mlflow/llm_finetuning_job_classification.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#     print(\"Could not automatically get SageMaker execution role. Please ensure it's configured or specify manually.\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://gom8fjg3ypjgkmk.studio.us-east-1.sagemaker.aws/home/sagemaker-user/job-classification-sagemaker-mlflow/llm_finetuning_job_classification.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#     raise ValueError(\"SageMaker execution role not found.\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://gom8fjg3ypjgkmk.studio.us-east-1.sagemaker.aws/home/sagemaker-user/job-classification-sagemaker-mlflow/llm_finetuning_job_classification.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m role \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marn:aws:iam::174671970284:role/service-role/AmazonSageMaker-ExecutionRole-20240216T153805\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://gom8fjg3ypjgkmk.studio.us-east-1.sagemaker.aws/home/sagemaker-user/job-classification-sagemaker-mlflow/llm_finetuning_job_classification.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m sess \u001b[39m=\u001b[39m sagemaker\u001b[39m.\u001b[39;49mSession()\n\u001b[1;32m     <a href='vscode-notebook-cell://gom8fjg3ypjgkmk.studio.us-east-1.sagemaker.aws/home/sagemaker-user/job-classification-sagemaker-mlflow/llm_finetuning_job_classification.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m region \u001b[39m=\u001b[39m sess\u001b[39m.\u001b[39mboto_region_name\n\u001b[1;32m     <a href='vscode-notebook-cell://gom8fjg3ypjgkmk.studio.us-east-1.sagemaker.aws/home/sagemaker-user/job-classification-sagemaker-mlflow/llm_finetuning_job_classification.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m default_bucket \u001b[39m=\u001b[39m sess\u001b[39m.\u001b[39mdefault_bucket()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:265\u001b[0m, in \u001b[0;36mSession.__init__\u001b[0;34m(self, boto_session, sagemaker_client, sagemaker_runtime_client, sagemaker_featurestore_runtime_client, default_bucket, settings, sagemaker_metrics_client, sagemaker_config, default_bucket_prefix)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlambda_client \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettings \u001b[39m=\u001b[39m settings \u001b[39mif\u001b[39;00m settings \u001b[39melse\u001b[39;00m SessionSettings()\n\u001b[0;32m--> 265\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(\n\u001b[1;32m    266\u001b[0m     boto_session\u001b[39m=\u001b[39;49mboto_session,\n\u001b[1;32m    267\u001b[0m     sagemaker_client\u001b[39m=\u001b[39;49msagemaker_client,\n\u001b[1;32m    268\u001b[0m     sagemaker_runtime_client\u001b[39m=\u001b[39;49msagemaker_runtime_client,\n\u001b[1;32m    269\u001b[0m     sagemaker_featurestore_runtime_client\u001b[39m=\u001b[39;49msagemaker_featurestore_runtime_client,\n\u001b[1;32m    270\u001b[0m     sagemaker_metrics_client\u001b[39m=\u001b[39;49msagemaker_metrics_client,\n\u001b[1;32m    271\u001b[0m     sagemaker_config\u001b[39m=\u001b[39;49msagemaker_config,\n\u001b[1;32m    272\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:346\u001b[0m, in \u001b[0;36mSession._initialize\u001b[0;34m(self, boto_session, sagemaker_client, sagemaker_runtime_client, sagemaker_featurestore_runtime_client, sagemaker_metrics_client, sagemaker_config)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_config \u001b[39m=\u001b[39m sagemaker_config\n\u001b[1;32m    343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m     \u001b[39m# self.s3_resource might be None. If it is None, load_sagemaker_config will\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     \u001b[39m# create a default S3 resource, but only if it needs to fetch from S3\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_config \u001b[39m=\u001b[39m load_sagemaker_config(s3_resource\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ms3_resource)\n\u001b[1;32m    348\u001b[0m \u001b[39m# after sagemaker_config initialization, update self._default_bucket_name_override if needed\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_bucket_name_override \u001b[39m=\u001b[39m resolve_value_from_config(\n\u001b[1;32m    350\u001b[0m     direct_input\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_bucket_name_override,\n\u001b[1;32m    351\u001b[0m     config_path\u001b[39m=\u001b[39mSESSION_DEFAULT_S3_BUCKET_PATH,\n\u001b[1;32m    352\u001b[0m     sagemaker_session\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m    353\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/config/config.py:128\u001b[0m, in \u001b[0;36mload_sagemaker_config\u001b[0;34m(additional_config_paths, s3_resource, repeat_log)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m         config_from_file \u001b[39m=\u001b[39m _load_config_from_file(file_path)\n\u001b[1;32m    129\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m    130\u001b[0m         \u001b[39mif\u001b[39;00m file_path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\n\u001b[1;32m    131\u001b[0m             _DEFAULT_ADMIN_CONFIG_FILE_PATH,\n\u001b[1;32m    132\u001b[0m             _DEFAULT_USER_CONFIG_FILE_PATH,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[39m# If there are no files in the Default config file locations, don't throw\u001b[39;00m\n\u001b[1;32m    136\u001b[0m             \u001b[39m# Exceptions.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/config/config.py:179\u001b[0m, in \u001b[0;36m_load_config_from_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    177\u001b[0m     inferred_file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(file_path, _CONFIG_FILE_NAME)\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(inferred_file_path):\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    180\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to load the config file from the location: \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProvide a valid file path\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m     )\n\u001b[1;32m    183\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mFetching defaults config from location: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, file_path)\n\u001b[1;32m    184\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(inferred_file_path, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to load the config file from the location: /home/sagemaker-user/job-classification-sagemaker-mlflowProvide a valid file path"
     ]
    }
   ],
   "source": [
    "# try:\n",
    "#     role = sagemaker.get_execution_role()\n",
    "#     print(f\"SageMaker Execution Role: {role}\")\n",
    "# except ValueError:\n",
    "#     iam = boto3.client(\"iam\")\n",
    "#     print(\"Could not automatically get SageMaker execution role. Please ensure it's configured or specify manually.\")\n",
    "#     raise ValueError(\"SageMaker execution role not found.\")\n",
    "\n",
    "role = \"arn:aws:iam::174671970284:role/service-role/AmazonSageMaker-ExecutionRole-20240216T153805\"\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "default_bucket = sess.default_bucket()\n",
    "print(f\"SageMaker Session region: {region}, bucket: {default_bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = \"JobDescClassification-Llama3-Pipeline-V5\" \n",
    "base_job_prefix = \"job-desc-classify\" \n",
    "\n",
    "mlflow_tracking_server_arn = \"arn:aws:sagemaker:your-region:your-aws-account-id:mlflow-tracking-server/your-tracking-server-name\" # <--- REPLACE THIS\n",
    "mlflow_experiment_name = \"JobDescriptionClassification-Llama3-FineTuning\"\n",
    "\n",
    "model_id_default = \"meta-llama/Meta-Llama-3-8B\" \n",
    "\n",
    "processed_data_s3_prefix = f\"{base_job_prefix}/processed_data/v3\" \n",
    "\n",
    "default_raw_data_s3_uri = f\"s3://{default_bucket}/raw_job_data/poc_multilingual_01/raw_jds_translated.jsonl\" \n",
    "\n",
    "preprocess_instance_type = \"ml.m5.large\"\n",
    "finetune_launcher_instance_type = \"ml.m5.large\" \n",
    "default_training_instance_type = \"ml.g5.12xlarge\" \n",
    "evaluation_instance_type = \"ml.g5.2xlarge\" \n",
    "\n",
    "default_hf_training_image_uri = f'763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-training:2.1.0-transformers4.36.0-gpu-py310-cu121-ubuntu20.04'\n",
    "\n",
    "if \"your-region\" in mlflow_tracking_server_arn:\n",
    "    print(\"ERROR: MLflow Tracking Server ARN is a placeholder. Please replace it.\")\n",
    "if \"raw_job_data/poc_multilingual_01/raw_jds_translated.jsonl\" in default_raw_data_s3_uri:\n",
    "    # This is a very basic check and might not be accurate for all S3 path styles\n",
    "    print(f\"WARNING: Default RawDatasetS3URI is set to '{default_raw_data_s3_uri}'. Ensure this S3 URI points to your generated raw dataset or override this parameter when starting the pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_raw_data_s3_uri = ParameterString(name=\"RawDatasetS3URI\", default_value=default_raw_data_s3_uri)\n",
    "param_job_desc_column = ParameterString(name=\"JobDescriptionColumn\", default_value=\"job_description_text\")\n",
    "param_category_column = ParameterString(name=\"CategoryColumn\", default_value=\"category_label\")\n",
    "param_test_split_fraction = ParameterFloat(name=\"TestSplitFraction\", default_value=0.15)\n",
    "param_validation_split_fraction = ParameterFloat(name=\"ValidationSplitFraction\", default_value=0.15)\n",
    "param_max_samples_per_split = ParameterInteger(name=\"MaxSamplesPerSplit\", default_value=-1)\n",
    "\n",
    "param_model_id = ParameterString(name=\"ModelIdentifier\", default_value=model_id_default)\n",
    "param_training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=default_training_instance_type)\n",
    "param_training_instance_count = ParameterInteger(name=\"TrainingInstanceCount\", default_value=1)\n",
    "param_hf_training_image_uri = ParameterString(name=\"HFTrainingImageURI\", default_value=default_hf_training_image_uri)\n",
    "\n",
    "param_finetune_epochs = ParameterInteger(name=\"FineTuneEpochs\", default_value=1)\n",
    "param_per_device_train_batch_size = ParameterInteger(name=\"PerDeviceTrainBatchSize\", default_value=1)\n",
    "param_learning_rate = ParameterFloat(name=\"LearningRate\", default_value=0.0002)\n",
    "param_lora_r = ParameterInteger(name=\"LoraR\", default_value=8)\n",
    "param_lora_alpha = ParameterInteger(name=\"LoraAlpha\", default_value=16)\n",
    "param_lora_dropout = ParameterFloat(name=\"LoraDropout\", default_value=0.05)\n",
    "param_lora_target_modules = ParameterString(name=\"LoraTargetModules\", default_value=\"q_proj,v_proj,k_proj,o_proj\")\n",
    "param_merge_weights = ParameterString(name=\"MergeWeights\", default_value=\"True\")\n",
    "param_hf_token = ParameterString(name=\"HuggingFaceToken\", default_value=\"OPTIONAL_HF_TOKEN_PLACEHOLDER\")\n",
    "\n",
    "param_eval_batch_size = ParameterInteger(name=\"EvaluationBatchSize\", default_value=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Pipeline Steps using `@step` decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A. Preprocessing Step\n",
    "@step(\n",
    "    name=\"PreprocessJobData\",\n",
    "    instance_type=preprocess_instance_type,\n",
    "    keep_alive_period_in_seconds=300\n",
    ")\n",
    "def sm_pipeline_preprocess_data_step(\n",
    "    raw_s3_uri: str,\n",
    "    s3_bucket: str,\n",
    "    s3_prefix: str,\n",
    "    jd_column: str,\n",
    "    cat_column: str,\n",
    "    test_frac_val: float,\n",
    "    val_frac_val: float,\n",
    "    max_samples_val: int,\n",
    "    mlflow_arn_val: str,\n",
    "    mlflow_exp_val: str,\n",
    "    exec_id: str,\n",
    "):\n",
    "    actual_max_samples = None if max_samples_val < 0 else max_samples_val\n",
    "    return preprocess_data(\n",
    "        raw_dataset_identifier=raw_s3_uri,\n",
    "        s3_output_bucket=s3_bucket,\n",
    "        s3_output_prefix=s3_prefix,\n",
    "        job_desc_column=jd_column,\n",
    "        category_column=cat_column,\n",
    "        test_split_fraction=test_frac_val,\n",
    "        validation_from_train_fraction=val_frac_val,\n",
    "        max_samples_per_split=actual_max_samples,\n",
    "        mlflow_arn=mlflow_arn_val,\n",
    "        experiment_name=mlflow_exp_val,\n",
    "        run_name=exec_id,\n",
    "    )\n",
    "\n",
    "# B. Fine-tuning Launcher Step\n",
    "@step(\n",
    "    name=\"LaunchHFFineTuning\",\n",
    "    instance_type=finetune_launcher_instance_type, \n",
    "    keep_alive_period_in_seconds=300 \n",
    ")\n",
    "def sm_pipeline_finetune_launcher_step(\n",
    "    processed_data_info_dict: dict,\n",
    "    sagemaker_iam_role: str,\n",
    "    train_instance_type_str: str,\n",
    "    train_instance_count_int: int,\n",
    "    hf_image_uri_str: str,\n",
    "    model_id_str: str,\n",
    "    epochs_int: int,\n",
    "    batch_size_int: int,\n",
    "    lr_float: float,\n",
    "    lora_r_int: int,\n",
    "    lora_alpha_int: int,\n",
    "    lora_dropout_float: float,\n",
    "    lora_targets_str: str,\n",
    "    merge_weights_str: str,\n",
    "    hf_token_str: str,\n",
    "    mlflow_arn_str: str,\n",
    "    mlflow_exp_str: str,\n",
    "    pipeline_exec_id_str: str,\n",
    "):\n",
    "    merge_weights_bool = merge_weights_str.lower() == 'true'\n",
    "    actual_hf_token = hf_token_str if hf_token_str and hf_token_str != \"OPTIONAL_HF_TOKEN_PLACEHOLDER\" else None\n",
    "    \n",
    "    # launch_hf_training_job is from steps.finetune_llama3_classifier (your launcher script)\n",
    "    # Ensure this launcher script internally sets source_dir=\"scripts\" and entry_point=\"python/finetune_entrypoint.py\"\n",
    "    return launch_hf_training_job(\n",
    "        role=sagemaker_iam_role,\n",
    "        image_uri=hf_image_uri_str,\n",
    "        instance_type=train_instance_type_str,\n",
    "        instance_count=train_instance_count_int,\n",
    "        train_s3_uri=processed_data_info_dict['train'],\n",
    "        validation_s3_uri=processed_data_info_dict['validation'],\n",
    "        # The following two are now expected to be hardcoded/managed within your launcher script:\n",
    "        entry_point_script=\"python/finetune_entrypoint.py\", # This should match what your launcher expects or uses internally\n",
    "        source_directory=\"scripts\", # This should match what your launcher expects or uses internally\n",
    "        model_id_hf=model_id_str,\n",
    "        epochs_val=epochs_int,\n",
    "        per_device_train_batch_size_val=batch_size_int,\n",
    "        learning_rate_val=lr_float,\n",
    "        lora_r_val=lora_r_int,\n",
    "        lora_alpha_val=lora_alpha_int,\n",
    "        lora_dropout_val=lora_dropout_float,\n",
    "        lora_target_modules_val=lora_targets_str,\n",
    "        merge_weights_val=merge_weights_bool,\n",
    "        hf_token_val=actual_hf_token,\n",
    "        mlflow_tracking_arn=mlflow_arn_str,\n",
    "        mlflow_experiment=mlflow_exp_str,\n",
    "        pipeline_run_id=pipeline_exec_id_str,\n",
    "        base_job_name_prefix=f\"job-clf-{model_id_str.split('/')[-1].replace('_','-')}\"\n",
    "    )\n",
    "\n",
    "# C. Evaluation Step\n",
    "@step(\n",
    "    name=\"EvaluateClassifier\",\n",
    "    instance_type=evaluation_instance_type,\n",
    "    image_uri=default_hf_training_image_uri, \n",
    "    keep_alive_period_in_seconds=600\n",
    ")\n",
    "def sm_pipeline_evaluate_model_step(\n",
    "    finetune_launcher_output_dict: dict, \n",
    "    processed_data_info_dict: dict, \n",
    "    eval_batch_size_int: int, \n",
    "    mlflow_arn_str: str,\n",
    "    mlflow_exp_str: str,\n",
    "    pipeline_exec_id_str: str \n",
    "):\n",
    "    mlflow_model_uri = f\"runs:/{pipeline_exec_id_str}/fine_tuned_classifier_model\" \n",
    "    \n",
    "    return evaluate_model(\n",
    "        model_s3_path_or_mlflow_uri=mlflow_model_uri, \n",
    "        test_data_s3_path=processed_data_info_dict['test'],\n",
    "        poc_categories_s3_path=processed_data_info_dict['categories_s3_path'],\n",
    "        batch_size=eval_batch_size_int,\n",
    "        mlflow_arn=mlflow_arn_str,\n",
    "        experiment_name=mlflow_exp_str,\n",
    "        run_id=pipeline_exec_id_str \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Construct the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_step_output_data = sm_pipeline_preprocess_data_step(\n",
    "    raw_s3_uri=param_raw_data_s3_uri,\n",
    "    s3_bucket=default_bucket, \n",
    "    s3_prefix=processed_data_s3_prefix,\n",
    "    jd_column=param_job_desc_column,\n",
    "    cat_column=param_category_column,\n",
    "    test_frac_val=param_test_split_fraction,\n",
    "    val_frac_val=param_validation_split_fraction,\n",
    "    max_samples_val=param_max_samples_per_split, \n",
    "    mlflow_arn_val=mlflow_tracking_server_arn, \n",
    "    mlflow_exp_val=mlflow_experiment_name, \n",
    "    exec_id=ExecutionVariables.PIPELINE_EXECUTION_ID\n",
    ")\n",
    "\n",
    "finetune_step_output_data = sm_pipeline_finetune_launcher_step(\n",
    "    processed_data_info_dict=preprocess_step_output_data, \n",
    "    sagemaker_iam_role=role, \n",
    "    train_instance_type_str=param_training_instance_type,\n",
    "    train_instance_count_int=param_training_instance_count,\n",
    "    hf_image_uri_str=param_hf_training_image_uri,\n",
    "    model_id_str=param_model_id, \n",
    "    epochs_int=param_finetune_epochs,\n",
    "    batch_size_int=param_per_device_train_batch_size,\n",
    "    lr_float=param_learning_rate,\n",
    "    lora_r_int=param_lora_r,\n",
    "    lora_alpha_int=param_lora_alpha,\n",
    "    lora_dropout_float=param_lora_dropout,\n",
    "    lora_targets_str=param_lora_target_modules,\n",
    "    merge_weights_str=param_merge_weights,\n",
    "    hf_token_str=param_hf_token,\n",
    "    mlflow_arn_str=mlflow_tracking_server_arn,\n",
    "    mlflow_exp_str=mlflow_experiment_name,\n",
    "    pipeline_exec_id_str=ExecutionVariables.PIPELINE_EXECUTION_ID\n",
    ")\n",
    "\n",
    "evaluate_step_output_data = sm_pipeline_evaluate_model_step(\n",
    "    finetune_launcher_output_dict=finetune_step_output_data, \n",
    "    processed_data_info_dict=preprocess_step_output_data, \n",
    "    eval_batch_size_int=param_eval_batch_size,\n",
    "    mlflow_arn_str=mlflow_tracking_server_arn,\n",
    "    mlflow_exp_str=mlflow_experiment_name,\n",
    "    pipeline_exec_id_str=ExecutionVariables.PIPELINE_EXECUTION_ID\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        param_raw_data_s3_uri,\n",
    "        param_job_desc_column,\n",
    "        param_category_column,\n",
    "        param_test_split_fraction,\n",
    "        param_validation_split_fraction,\n",
    "        param_max_samples_per_split,\n",
    "        param_model_id,\n",
    "        param_training_instance_type,\n",
    "        param_training_instance_count,\n",
    "        param_hf_training_image_uri,\n",
    "        param_finetune_epochs,\n",
    "        param_per_device_train_batch_size,\n",
    "        param_learning_rate,\n",
    "        param_lora_r,\n",
    "        param_lora_alpha,\n",
    "        param_lora_dropout,\n",
    "        param_lora_target_modules,\n",
    "        param_merge_weights,\n",
    "        param_hf_token,\n",
    "        param_eval_batch_size\n",
    "    ],\n",
    "    steps=[evaluate_step_output_data],\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Upsert and Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"your-region\" in mlflow_tracking_server_arn:\n",
    "    print(\"ERROR: MLflow Tracking Server ARN is a placeholder. Update it in cell [3].\")\n",
    "else:\n",
    "    print(\"\\nUpserting the pipeline...\")\n",
    "    try:\n",
    "        pipeline.upsert(role_arn=role)\n",
    "        print(f\"Pipeline '{pipeline_name}' upserted successfully.\")\n",
    "\n",
    "        print(\"\\nStarting pipeline execution...\")\n",
    "        execution = pipeline.start(\n",
    "            parameters={}\n",
    "        )\n",
    "        print(f\"Pipeline execution started with ARN: {execution.arn}\")\n",
    "        execution.describe()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during pipeline upsert or start: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Clean up (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To delete the pipeline definition from SageMaker:\n",
    "# try:\n",
    "#     pipeline.delete()\n",
    "#     print(f\"Pipeline '{pipeline_name}' deleted.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error deleting pipeline '{pipeline_name}': {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "sagemaker": {
   "image_uri": "arn:aws:sagemaker:us-east-1:000000000000:image/sagemaker-data-science-310-v1",
   "instance_type": "ml.t3.medium"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

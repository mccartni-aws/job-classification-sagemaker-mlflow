{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Fine-Tuning for Job Description Classification with SageMaker & MLflow\n",
    "\n",
    "This notebook demonstrates fine-tuning Llama 3 for job description classification using SageMaker Pipelines and MLflow for experiment tracking. It assumes raw data has been pre-generated and uploaded to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f14bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting fsspec==2023.6.0\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec\n",
      "Successfully installed fsspec-2023.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --ignore-installed --no-deps --no-cache-dir fsspec==2023.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f31facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker==2.225.0  datasets transformers>=4.40.0 mlflow peft>=0.9.0 sagemaker-mlflow --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.7.0+cu118 available.\n",
      "INFO:datasets:TensorFlow version 2.12.1 available.\n",
      "INFO:datasets:JAX version 0.4.20 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to sys.path: /home/sagemaker-user/job-classification-sagemaker-mlflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:18:46.369485: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'EncoderDecoderCache' from 'transformers' (/opt/conda/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/sagemaker-user/job-classification-sagemaker-mlflow/notebooks/llm_fine_tuning_job_classification_mlflow2.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://gom8fjg3ypjgkmk.studio.us-east-1.sagemaker.aws/home/sagemaker-user/job-classification-sagemaker-mlflow/notebooks/llm_fine_tuning_job_classification_mlflow2.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# These imports refer to the scripts in your 'steps/' directory.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://gom8fjg3ypjgkmk.studio.us-east-1.sagemaker.aws/home/sagemaker-user/job-classification-sagemaker-mlflow/notebooks/llm_fine_tuning_job_classification_mlflow2.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msteps\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocess_job_descriptions\u001b[39;00m \u001b[39mimport\u001b[39;00m preprocess_data\n\u001b[0;32m---> <a href='vscode-notebook-cell://gom8fjg3ypjgkmk.studio.us-east-1.sagemaker.aws/home/sagemaker-user/job-classification-sagemaker-mlflow/notebooks/llm_fine_tuning_job_classification_mlflow2.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msteps\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfinetune_llama3_classifier\u001b[39;00m \u001b[39mimport\u001b[39;00m finetune_model \u001b[39m# Assuming this is the main function in your finetune script\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://gom8fjg3ypjgkmk.studio.us-east-1.sagemaker.aws/home/sagemaker-user/job-classification-sagemaker-mlflow/notebooks/llm_fine_tuning_job_classification_mlflow2.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msteps\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mevaluation_classifier\u001b[39;00m \u001b[39mimport\u001b[39;00m evaluate_model\n\u001b[1;32m     <a href='vscode-notebook-cell://gom8fjg3ypjgkmk.studio.us-east-1.sagemaker.aws/home/sagemaker-user/job-classification-sagemaker-mlflow/notebooks/llm_fine_tuning_job_classification_mlflow2.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mSAGEMAKER_USER_CONFIG_OVERRIDE\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetcwd()\n",
      "File \u001b[0;32m~/job-classification-sagemaker-mlflow/steps/finetune_llama3_classifier.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, pipeline\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpeft\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftModel \u001b[39m# If loading adapters separately\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score, classification_report, confusion_matrix\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2023-present the HuggingFace Inc. team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0.15.2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     MODEL_TYPE_TO_PEFT_MODEL_MAPPING,\n\u001b[1;32m     19\u001b[0m     AutoPeftModel,\n\u001b[1;32m     20\u001b[0m     AutoPeftModelForCausalLM,\n\u001b[1;32m     21\u001b[0m     AutoPeftModelForFeatureExtraction,\n\u001b[1;32m     22\u001b[0m     AutoPeftModelForQuestionAnswering,\n\u001b[1;32m     23\u001b[0m     AutoPeftModelForSeq2SeqLM,\n\u001b[1;32m     24\u001b[0m     AutoPeftModelForSequenceClassification,\n\u001b[1;32m     25\u001b[0m     AutoPeftModelForTokenClassification,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftConfig, PromptLearningConfig\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmapping\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     PEFT_TYPE_TO_CONFIG_MAPPING,\n\u001b[1;32m     30\u001b[0m     PEFT_TYPE_TO_MIXED_MODEL_MAPPING,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     inject_adapter_in_model,\n\u001b[1;32m     34\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/auto.py:32\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     AutoModel,\n\u001b[1;32m     23\u001b[0m     AutoModelForCausalLM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     AutoTokenizer,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftConfig\n\u001b[0;32m---> 32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpeft_model\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     PeftModel,\n\u001b[1;32m     34\u001b[0m     PeftModelForCausalLM,\n\u001b[1;32m     35\u001b[0m     PeftModelForFeatureExtraction,\n\u001b[1;32m     36\u001b[0m     PeftModelForQuestionAnswering,\n\u001b[1;32m     37\u001b[0m     PeftModelForSeq2SeqLM,\n\u001b[1;32m     38\u001b[0m     PeftModelForSequenceClassification,\n\u001b[1;32m     39\u001b[0m     PeftModelForTokenClassification,\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconstants\u001b[39;00m \u001b[39mimport\u001b[39;00m TOKENIZER_CONFIG_NAME\n\u001b[1;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mother\u001b[39;00m \u001b[39mimport\u001b[39;00m check_file_exists_on_hf_hub\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msafetensors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m save_file \u001b[39mas\u001b[39;00m safe_save_file\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m Cache, DynamicCache, EncoderDecoderCache, PreTrainedModel\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_outputs\u001b[39;00m \u001b[39mimport\u001b[39;00m QuestionAnsweringModelOutput, SequenceClassifierOutput, TokenClassifierOutput\n\u001b[1;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m PushToHubMixin\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'EncoderDecoderCache' from 'transformers' (/opt/conda/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "import json\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.function_step import step\n",
    "from sagemaker.workflow.parameters import ParameterString, ParameterInteger, ParameterFloat\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "\n",
    "import sys\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if project_root not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root}\")\n",
    "    sys.path.insert(0, project_root)\n",
    "else:\n",
    "    print(f\"Project root already in sys.path: {project_root}\")\n",
    "\n",
    "# These imports refer to the scripts in your 'steps/' directory.\n",
    "from steps.preprocess_job_descriptions import preprocess_data\n",
    "from steps.finetune_llama3_classifier import finetune_model # Assuming this is the main function in your finetune script\n",
    "from steps.evaluation_classifier import evaluate_model\n",
    "\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SageMaker Session and IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    print(f\"SageMaker Execution Role: {role}\")\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    print(\"Could not automatically get SageMaker execution role. Please ensure it's configured or specify manually.\")\n",
    "    # Fallback: Replace with your specific role name if necessary\n",
    "    # role_name = \"YourSageMakerExecutionRoleName\" \n",
    "    # role = iam.get_role(RoleName=role_name)[\"Role\"][\"Arn\"]\n",
    "    raise ValueError(\"SageMaker execution role not found. Please create one or ensure your environment is configured.\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "default_bucket = sess.default_bucket()\n",
    "print(f\"SageMaker Session region: {region}, bucket: {default_bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = \"JobDescClassification-Llama3-Pipeline-V4\" \n",
    "base_job_prefix = \"job-desc-classify\" \n",
    "\n",
    "mlflow_tracking_server_arn = \"arn:aws:sagemaker:your-region:your-aws-account-id:mlflow-tracking-server/your-tracking-server-name\" # <--- REPLACE THIS\n",
    "mlflow_experiment_name = \"JobDescriptionClassification-Llama3-FineTuning\"\n",
    "\n",
    "model_id_default = \"meta-llama/Meta-Llama-3-8B\" \n",
    "\n",
    "processed_data_s3_prefix = f\"{base_job_prefix}/processed_data/v2\" # Example prefix for processed data\n",
    "\n",
    "preprocess_instance_type = \"ml.m5.large\"\n",
    "finetune_instance_type = \"ml.g5.12xlarge\" \n",
    "evaluation_instance_type = \"ml.g5.2xlarge\" \n",
    "\n",
    "if \"your-region\" in mlflow_tracking_server_arn:\n",
    "    print(\"WARNING: MLflow Tracking Server ARN is a placeholder. Please replace it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_raw_data_s3_uri = ParameterString(\n",
    "    name=\"RawDatasetS3URI\", \n",
    "    default_value=f\"s3://{default_bucket}/raw_job_description_data/v1_translated/raw_jds_translated.jsonl\" # Example default\n",
    ")\n",
    "param_job_desc_column = ParameterString(name=\"JobDescriptionColumn\", default_value=\"job_description_text\")\n",
    "param_category_column = ParameterString(name=\"CategoryColumn\", default_value=\"category_label\")\n",
    "param_test_split_fraction = ParameterFloat(name=\"TestSplitFraction\", default_value=0.15)\n",
    "param_validation_split_fraction = ParameterFloat(name=\"ValidationSplitFraction\", default_value=0.15)\n",
    "param_max_samples_per_split = ParameterInteger(name=\"MaxSamplesPerSplit\", default_value=-1) # -1 means no limit, use 0 or None in script\n",
    "\n",
    "param_model_id = ParameterString(name=\"ModelIdentifier\", default_value=model_id_default)\n",
    "param_finetune_epochs = ParameterInteger(name=\"FineTuneEpochs\", default_value=1)\n",
    "param_per_device_train_batch_size = ParameterInteger(name=\"PerDeviceTrainBatchSize\", default_value=1)\n",
    "param_lora_r = ParameterInteger(name=\"LoraR\", default_value=8)\n",
    "param_lora_alpha = ParameterInteger(name=\"LoraAlpha\", default_value=16)\n",
    "param_lora_dropout = ParameterFloat(name=\"LoraDropout\", default_value=0.05)\n",
    "param_learning_rate = ParameterFloat(name=\"LearningRate\", default_value=0.0002)\n",
    "param_merge_weights = ParameterString(name=\"MergeWeights\", default_value=\"True\") # Use String for boolean-like params\n",
    "param_hf_token = ParameterString(name=\"HuggingFaceToken\", default_value=\"OPTIONAL_HF_TOKEN_PLACEHOLDER\")\n",
    "\n",
    "param_eval_batch_size = ParameterInteger(name=\"EvaluationBatchSize\", default_value=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Pipeline Steps using `@step` decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from steps.finetune_llama3_classifier import launch_hf_training_job\n",
    "\n",
    "# A. Preprocessing Step\n",
    "@step(\n",
    "    name=\"PreprocessRawJobData\",\n",
    "    instance_type=preprocess_instance_type,\n",
    "    keep_alive_period_in_seconds=300\n",
    ")\n",
    "def sm_pipeline_preprocess_step(\n",
    "    raw_data_s3_identifier: str,\n",
    "    output_s3_bucket_name: str,\n",
    "    output_s3_prefix_val: str,\n",
    "    jd_col: str,\n",
    "    cat_col: str,\n",
    "    test_frac: float,\n",
    "    val_frac: float,\n",
    "    max_samples: int, # Corresponds to max_samples_per_split\n",
    "    mlflow_arn_tracking: str,\n",
    "    mlflow_exp_name: str,\n",
    "    pipeline_exec_id: str,\n",
    "):\n",
    "    # The imported 'preprocess_data' is from 'steps/preprocess_job_descriptions.py'\n",
    "    max_samples_val = None if max_samples < 0 else max_samples # Convert -1 to None for the script\n",
    "    s3_paths_and_run_id = preprocess_data(\n",
    "        raw_dataset_identifier=raw_data_s3_identifier,\n",
    "        s3_output_bucket=output_s3_bucket_name,\n",
    "        s3_output_prefix=output_s3_prefix_val,\n",
    "        job_desc_column=jd_col,\n",
    "        category_column=cat_col,\n",
    "        test_split_fraction=test_frac,\n",
    "        validation_from_train_fraction=val_frac,\n",
    "        max_samples_per_split=max_samples_val,\n",
    "        mlflow_arn=mlflow_arn_tracking,\n",
    "        experiment_name=mlflow_exp_name,\n",
    "        run_name=pipeline_exec_id,\n",
    "    )\n",
    "    return s3_paths_and_run_id\n",
    "\n",
    "# B. Fine-tuning Step\n",
    "hf_pytorch_image_uri = sagemaker.image_uris.retrieve(\n",
    "    \"huggingface-pytorch-training\",\n",
    "    region=region,\n",
    "    version=\"4.31.0\", \n",
    "    py_version=\"py310\",\n",
    "    instance_type=finetune_instance_type, \n",
    "    image_scope=\"training\"\n",
    ")\n",
    "print(f\"Using HuggingFace PyTorch image for fine-tuning: {hf_pytorch_image_uri}\")\n",
    "\n",
    "@step(\n",
    "    name=\"FineTuneLlama3Classifier\",\n",
    "    instance_type=finetune_instance_type,\n",
    "    image_uri=hf_pytorch_image_uri, \n",
    "    keep_alive_period_in_seconds=3600 \n",
    ")\n",
    "def sm_pipeline_finetune_step(\n",
    "    processed_data_info: dict, \n",
    "    model_identifier_str: str,\n",
    "    epochs_ft: int,\n",
    "    batch_size_ft: int,\n",
    "    lora_r_val: int,\n",
    "    lora_alpha_val: int,\n",
    "    lora_dropout_val: float,\n",
    "    lr_val: float,\n",
    "    merge_w: str, # String 'True' or 'False'\n",
    "    mlflow_arn_tracking: str,\n",
    "    mlflow_exp_name: str,\n",
    "    pipeline_exec_id: str,\n",
    "    hf_auth_token: str\n",
    "):\n",
    "    # The imported 'finetune_model' is from 'steps/finetune_llama3_classifier.py'\n",
    "    actual_hf_token = hf_auth_token if hf_auth_token and hf_auth_token != \"OPTIONAL_HF_TOKEN_PLACEHOLDER\" else os.environ.get(\"HF_TOKEN\")\n",
    "    merge_weights_bool = merge_w.lower() == 'true'\n",
    "    \n",
    "    # finetune_model script saves to /opt/ml/model by default.\n",
    "    # The S3 path of this uploaded model artifact will be the output of this step.\n",
    "    local_model_output_path = finetune_model(\n",
    "        model_id=model_identifier_str,\n",
    "        train_data_s3_path=processed_data_info['train'],\n",
    "        eval_data_s3_path=processed_data_info['validation'],\n",
    "        # output_dir is handled by the script, defaults to /opt/ml/model\n",
    "        epochs=epochs_ft,\n",
    "        per_device_train_batch_size=batch_size_ft,\n",
    "        learning_rate=lr_val,\n",
    "        lora_r=lora_r_val,\n",
    "        lora_alpha=lora_alpha_val,\n",
    "        lora_dropout=lora_dropout_val,\n",
    "        merge_weights=merge_weights_bool,\n",
    "        hf_token=actual_hf_token,\n",
    "        mlflow_arn=mlflow_arn_tracking,\n",
    "        experiment_name=mlflow_exp_name,\n",
    "        run_id=pipeline_exec_id \n",
    "    )\n",
    "    # The @step decorator uploads /opt/ml/model. We return a dict for clarity for the next step.\n",
    "    return {\"model_s3_path_implicit\": \"s3_path_managed_by_sagemaker_for_opt_ml_model\", \"mlflow_run_id\": pipeline_exec_id} \n",
    "\n",
    "# C. Evaluation Step\n",
    "@step(\n",
    "    name=\"EvaluateFineTunedClassifier\",\n",
    "    instance_type=evaluation_instance_type,\n",
    "    image_uri=hf_pytorch_image_uri,\n",
    "    keep_alive_period_in_seconds=600\n",
    ")\n",
    "def sm_pipeline_evaluate_step(\n",
    "    finetune_step_output: dict, # Contains mlflow_run_id for constructing MLflow model URI\n",
    "    processed_data_info: dict, # Contains 'test' and 'categories_s3_path'\n",
    "    eval_bs: int, \n",
    "    mlflow_arn_tracking: str,\n",
    "    mlflow_exp_name: str,\n",
    "    pipeline_exec_id: str # This is the overall pipeline execution ID\n",
    "):\n",
    "    # The imported 'evaluate_model' is from 'steps/evaluation_classifier.py'\n",
    "    # Construct MLflow model URI from the pipeline_exec_id (which was used as run_id for finetuning)\n",
    "    # Assuming finetune_model logged model as 'fine_tuned_classifier_model' artifact\n",
    "    mlflow_model_uri_to_load = f\"runs:/{pipeline_exec_id}/fine_tuned_classifier_model\" \n",
    "    \n",
    "    eval_results = evaluate_model(\n",
    "        model_s3_path_or_mlflow_uri=mlflow_model_uri_to_load, \n",
    "        test_data_s3_path=processed_data_info['test'],\n",
    "        poc_categories_s3_path=processed_data_info['categories_s3_path'],\n",
    "        batch_size=eval_bs,\n",
    "        mlflow_arn=mlflow_arn_tracking,\n",
    "        experiment_name=mlflow_exp_name,\n",
    "        run_id=pipeline_exec_id # Evaluate logs under the same parent run_id\n",
    "    )\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Construct the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_outputs = sm_pipeline_preprocess_step(\n",
    "    raw_data_s3_identifier=param_raw_data_s3_uri,\n",
    "    output_s3_bucket_name=default_bucket, \n",
    "    output_s3_prefix_val=processed_data_s3_prefix,\n",
    "    jd_col=param_job_desc_column,\n",
    "    cat_col=param_category_column,\n",
    "    test_frac=param_test_split_fraction,\n",
    "    val_frac=param_validation_split_fraction,\n",
    "    max_samples=param_max_samples_per_split, \n",
    "    mlflow_arn_tracking=mlflow_tracking_server_arn, \n",
    "    mlflow_exp_name=mlflow_experiment_name, \n",
    "    pipeline_exec_id=ExecutionVariables.PIPELINE_EXECUTION_ID\n",
    ")\n",
    "\n",
    "finetune_outputs = sm_pipeline_finetune_step(\n",
    "    processed_data_info=preprocess_outputs, \n",
    "    model_identifier_str=param_model_id, \n",
    "    epochs_ft=param_finetune_epochs,\n",
    "    batch_size_ft=param_per_device_train_batch_size,\n",
    "    lora_r_val=param_lora_r,\n",
    "    lora_alpha_val=param_lora_alpha,\n",
    "    lora_dropout_val=param_lora_dropout,\n",
    "    lr_val=param_learning_rate,\n",
    "    merge_w=param_merge_weights,\n",
    "    mlflow_arn_tracking=mlflow_tracking_server_arn,\n",
    "    mlflow_exp_name=mlflow_experiment_name,\n",
    "    pipeline_exec_id=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "    hf_auth_token=param_hf_token\n",
    ")\n",
    "\n",
    "evaluate_outputs = sm_pipeline_evaluate_step(\n",
    "    finetune_step_output=finetune_outputs, \n",
    "    processed_data_info=preprocess_outputs, \n",
    "    eval_bs=param_eval_batch_size,\n",
    "    mlflow_arn_tracking=mlflow_tracking_server_arn,\n",
    "    mlflow_exp_name=mlflow_experiment_name,\n",
    "    pipeline_exec_id=ExecutionVariables.PIPELINE_EXECUTION_ID\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        param_raw_data_s3_uri,\n",
    "        param_job_desc_column,\n",
    "        param_category_column,\n",
    "        param_test_split_fraction,\n",
    "        param_validation_split_fraction,\n",
    "        param_max_samples_per_split,\n",
    "        param_model_id,\n",
    "        param_finetune_epochs,\n",
    "        param_per_device_train_batch_size,\n",
    "        param_lora_r,\n",
    "        param_lora_alpha,\n",
    "        param_lora_dropout,\n",
    "        param_learning_rate,\n",
    "        param_merge_weights,\n",
    "        param_hf_token,\n",
    "        param_eval_batch_size\n",
    "    ],\n",
    "    steps=[evaluate_outputs],\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Upsert and Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"your-region\" in mlflow_tracking_server_arn:\n",
    "    print(\"ERROR: MLflow Tracking Server ARN is a placeholder. Update it in cell [3].\")\n",
    "elif \"raw_job_description_data/v1_translated/raw_jds_translated.jsonl\" in param_raw_data_s3_uri.default_value:\n",
    "    print(f\"INFO: Using default RawDatasetS3URI: '{param_raw_data_s3_uri.default_value}'.\")\n",
    "    print(\"       Ensure this S3 URI points to your generated raw dataset or override it when starting the pipeline.\")\n",
    "    print(\"       Run 'generate_and_upload_raw_data.py' script if you haven't already.\")\n",
    "\n",
    "print(\"\\nUpserting the pipeline...\")\n",
    "try:\n",
    "    pipeline.upsert(role_arn=role)\n",
    "    print(f\"Pipeline '{pipeline_name}' upserted successfully.\")\n",
    "\n",
    "    print(\"\\nStarting pipeline execution with default parameters...\")\n",
    "    # To override parameters:\n",
    "    # execution = pipeline.start(\n",
    "    #     parameters={\n",
    "    #         \"RawDatasetS3URI\": \"s3://your-bucket/your-actual-raw-data.jsonl\",\n",
    "    #         \"FineTuneEpochs\": 2\n",
    "    #     }\n",
    "    # )\n",
    "    execution = pipeline.start()\n",
    "    \n",
    "    print(f\"Pipeline execution started with ARN: {execution.arn}\")\n",
    "    execution.describe()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during pipeline upsert or start: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Clean up (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To delete the pipeline definition from SageMaker:\n",
    "# try:\n",
    "#     pipeline.delete()\n",
    "#     print(f\"Pipeline '{pipeline_name}' deleted.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error deleting pipeline '{pipeline_name}': {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "sagemaker": {
   "image_uri": "arn:aws:sagemaker:us-east-1:000000000000:image/sagemaker-data-science-310-v1",
   "instance_type": "ml.t3.medium"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

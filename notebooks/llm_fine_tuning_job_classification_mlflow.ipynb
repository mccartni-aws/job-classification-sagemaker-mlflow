{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Fine-Tuning for Job Description Classification with SageMaker & MLflow\n",
    "\n",
    "This notebook demonstrates fine-tuning Llama 3 for job description classification using SageMaker Pipelines and MLflow for experiment tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.200.0\" \"datasets>=2.18.0\" \"transformers>=4.38.0\" \"mlflow>=2.9.0\" \"sagemaker-mlflow>=0.1.0\" --quiet\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --quiet # Adjust cuXXX version if needed\n",
    "!pip install \"accelerate>=0.28.0\" \"bitsandbytes>=0.42.0\" \"scikit-learn\" \"pandas\" \"matplotlib\" \"seaborn\" \"huggingface_hub\" \"s3fs\" --quiet # s3fs for datasets from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "import json\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep \n",
    "from sagemaker.workflow.function_step import step\n",
    "from sagemaker.workflow.parameters import ParameterString, ParameterInteger, ParameterFloat\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# Ensure these .py files are in a 'steps' subdirectory relative to the notebook,\n",
    "# or adjust the path accordingly. Python sys.path might need adjustment if running locally\n",
    "# and 'steps' isn't automatically discoverable.\n",
    "# For SageMaker pipeline execution, the source_dir parameter for @step or ScriptProcessor handles this.\n",
    "from steps.preprocess_job_descriptions import preprocess_data\n",
    "from steps.finetune_llama3_classifier import finetune_model\n",
    "from steps.evaluation_classifier import evaluate_model\n",
    "\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd() # For custom images/config if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SageMaker Session and IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    print(f\"SageMaker Execution Role: {role}\")\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    # Option 1: Replace with your specific role name if not using the default pattern\n",
    "    # role_name = \"YourSageMakerExecutionRoleName\"\n",
    "    # role = iam.get_role(RoleName=role_name)[\"Role\"][\"Arn\"]\n",
    "    \n",
    "    # Option 2: Try to find a role with 'AmazonSageMaker-ExecutionRole' in its name (less reliable)\n",
    "    print(\"Could not automatically get SageMaker execution role. Please ensure it's configured or specify manually.\")\n",
    "    print(\"Attempting to find a suitable role...\")\n",
    "    roles = iam.list_roles(MaxItems=200) # List some roles\n",
    "    sagemaker_roles = [r['Arn'] for r in roles['Roles'] if 'AmazonSageMaker-ExecutionRole' in r['RoleName']]\n",
    "    if sagemaker_roles:\n",
    "        role = sagemaker_roles[0] # Take the first one found\n",
    "        print(f\"Found and using role: {role}\")\n",
    "    else:\n",
    "        raise ValueError(\"SageMaker execution role not found. Please create one or specify its ARN.\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "default_bucket = sess.default_bucket()\n",
    "print(f\"SageMaker Session region: {region}, bucket: {default_bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline and Experiment Configuration\n",
    "pipeline_name = \"JobDescClassification-Llama3-Pipeline-V2\" # Added V2 to distinguish\n",
    "base_job_prefix = \"job-desc-classify\" # Base prefix for SageMaker jobs\n",
    "\n",
    "# MLflow Configuration - **REPLACE WITH YOUR MLFLOW TRACKING SERVER ARN**\n",
    "mlflow_tracking_server_arn = \"arn:aws:sagemaker:your-region:your-aws-account-id:mlflow-tracking-server/your-tracking-server-name\" # <--- REPLACE THIS\n",
    "mlflow_experiment_name = \"JobDescriptionClassification-Llama3-FineTuning\"\n",
    "\n",
    "# Model Configuration\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\" \n",
    "\n",
    "# Data Configuration\n",
    "output_s3_data_prefix = f\"s3://{default_bucket}/{base_job_prefix}/data\" # Used by preprocess script\n",
    "\n",
    "# Instance Configuration for Pipeline Steps\n",
    "preprocess_instance_type = \"ml.m5.large\"\n",
    "finetune_instance_type = \"ml.g5.12xlarge\" # For Llama 3 8B QLoRA\n",
    "evaluation_instance_type = \"ml.g5.2xlarge\" \n",
    "\n",
    "# Check if MLflow ARN is a placeholder\n",
    "if \"your-region\" in mlflow_tracking_server_arn:\n",
    "    print(\"WARNING: MLflow Tracking Server ARN is a placeholder. Please replace it with your actual ARN.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_num_samples_per_category = ParameterInteger(name=\"NumSamplesPerCategory\", default_value=75)\n",
    "param_languages = ParameterString(name=\"Languages\", default_value=\"en,es\") \n",
    "param_finetune_epochs = ParameterInteger(name=\"FineTuneEpochs\", default_value=1)\n",
    "param_lora_r = ParameterInteger(name=\"LoraR\", default_value=8)\n",
    "param_lora_alpha = ParameterInteger(name=\"LoraAlpha\", default_value=16)\n",
    "param_learning_rate = ParameterFloat(name=\"LearningRate\", default_value=0.0002) # 2e-4\n",
    "\n",
    "# Parameter for Hugging Face Token (optional, can be handled via environment variables too)\n",
    "# If you set a default value other than a placeholder, be mindful of security.\n",
    "param_hf_token = ParameterString(name=\"HuggingFaceToken\", default_value=\"OPTIONAL_HF_TOKEN_PLACEHOLDER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Pipeline Steps using `@step` decorator\n",
    "\n",
    "The `@step` decorator packages the Python functions and their dependencies. Ensure the `steps` directory (containing `preprocess_job_descriptions.py`, etc.) is in the same directory as this notebook, or adjust `source_dir` if needed.\n",
    "\n",
    "For steps requiring specific libraries not in the default SageMaker image (especially GPU libraries like `bitsandbytes`), a custom Docker image or a pre-built SageMaker Deep Learning Container (DLC) image specified via `image_uri` in `@step` is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A. Preprocessing Step\n",
    "@step(\n",
    "    name=\"PreprocessJobDescriptions\",\n",
    "    instance_type=preprocess_instance_type,\n",
    "    keep_alive_period_in_seconds=300,\n",
    "    # 'dependencies=\"auto\"' can work for simple cases.\n",
    "    # For more complex dependencies or to ensure the 'steps' module is found:\n",
    "    # source_dir='./', # Or path to a dir containing 'steps' and any requirements.txt\n",
    "    # entry_point='steps/preprocess_job_descriptions.py', # if not directly calling the imported function\n",
    "    # For @step with direct function call, it usually infers from the function's module.\n",
    ")\n",
    "def pipeline_preprocess_step(\n",
    "    output_s3_bucket_name: str,\n",
    "    num_samples: int,\n",
    "    langs: str,\n",
    "    mlflow_arn_tracking: str,\n",
    "    mlflow_exp_name: str,\n",
    "    pipeline_exec_id: str,\n",
    "):\n",
    "    # Imports are typically inside the function for @step to package them correctly if not using source_dir/entry_point\n",
    "    # However, since we imported them at the top and they are in 'steps/', it should work.\n",
    "    # from steps.preprocess_job_descriptions import preprocess_data \n",
    "    \n",
    "    print(f\"Executing preprocessing. Output bucket: {output_s3_bucket_name}, Samples/cat: {num_samples}, Langs: {langs}\")\n",
    "    print(f\"MLflow ARN: {mlflow_arn_tracking}, Experiment: {mlflow_exp_name}, Run: {pipeline_exec_id}\")\n",
    "    \n",
    "    languages_list = [lang.strip() for lang in langs.split(',')]\n",
    "    s3_paths = preprocess_data(\n",
    "        output_s3_bucket=output_s3_bucket_name,\n",
    "        num_samples_per_category=num_samples,\n",
    "        mlflow_arn=mlflow_arn_tracking,\n",
    "        experiment_name=mlflow_exp_name,\n",
    "        run_name=pipeline_exec_id, \n",
    "        languages=languages_list\n",
    "    )\n",
    "    print(f\"Preprocessing output S3 paths: {s3_paths}\")\n",
    "    return s3_paths\n",
    "\n",
    "# B. Fine-tuning Step\n",
    "hf_pytorch_image_uri = sagemaker.image_uris.retrieve(\n",
    "    \"huggingface-pytorch-training\",\n",
    "    region=region,\n",
    "    version=\"4.31.0\", # Check for latest compatible version for Transformers 4.38+\n",
    "    py_version=\"py310\",\n",
    "    instance_type=finetune_instance_type, \n",
    "    image_scope=\"training\"\n",
    ")\n",
    "print(f\"Using HuggingFace PyTorch image for fine-tuning: {hf_pytorch_image_uri}\")\n",
    "\n",
    "@step(\n",
    "    name=\"FineTuneLlama3Classifier\",\n",
    "    instance_type=finetune_instance_type,\n",
    "    image_uri=hf_pytorch_image_uri, \n",
    "    keep_alive_period_in_seconds=3600, # Longer for training, e.g., 1 hour\n",
    "    # environment variable for Hugging Face token. The step function will receive it as parameter.\n",
    ")\n",
    "def pipeline_finetune_step(\n",
    "    processed_data_s3_paths: dict, \n",
    "    model_identifier: str,\n",
    "    epochs_ft: int,\n",
    "    lora_r_val: int,\n",
    "    lora_alpha_val: int,\n",
    "    lr_val: float,\n",
    "    mlflow_arn_tracking: str,\n",
    "    mlflow_exp_name: str,\n",
    "    pipeline_exec_id: str,\n",
    "    hf_auth_token: str # Parameter for HF token\n",
    "):\n",
    "    # from steps.finetune_llama3_classifier import finetune_model\n",
    "    \n",
    "    print(f\"Executing fine-tuning. Model: {model_identifier}, Epochs: {epochs_ft}, LoraR: {lora_r_val}\")\n",
    "    print(f\"MLflow ARN: {mlflow_arn_tracking}, Experiment: {mlflow_exp_name}, Run: {pipeline_exec_id}\")\n",
    "    \n",
    "    # Use the HF token if provided and not the placeholder\n",
    "    actual_hf_token = None\n",
    "    if hf_auth_token and hf_auth_token != \"OPTIONAL_HF_TOKEN_PLACEHOLDER\":\n",
    "        actual_hf_token = hf_auth_token\n",
    "    elif os.environ.get(\"HF_TOKEN\"):\n",
    "         actual_hf_token = os.environ.get(\"HF_TOKEN\") # If set in environment\n",
    "    \n",
    "    if not actual_hf_token and \"meta-llama\" in model_identifier:\n",
    "        print(\"WARNING: Hugging Face token not provided for a gated model. Fine-tuning might fail.\")\n",
    "        \n",
    "    # The finetune_model script saves to /opt/ml/model by default.\n",
    "    # SageMaker @step automatically uploads contents of /opt/ml/model to an S3 location.\n",
    "    # The S3 path of this uploaded model artifact will be the output of this step.\n",
    "    # The function itself can return the local path, and @step handles the S3 part.\n",
    "    local_model_output_path = finetune_model(\n",
    "        model_id=model_identifier,\n",
    "        train_data_s3_path=processed_data_s3_paths['train'],\n",
    "        eval_data_s3_path=processed_data_s3_paths['validation'],\n",
    "        epochs=epochs_ft,\n",
    "        per_device_train_batch_size=1, # Keep small for Llama3-8B on g5.12xl with QLoRA\n",
    "        learning_rate=lr_val,\n",
    "        lora_r=lora_r_val,\n",
    "        lora_alpha=lora_alpha_val,\n",
    "        mlflow_arn=mlflow_arn_tracking,\n",
    "        experiment_name=mlflow_exp_name,\n",
    "        run_id=pipeline_exec_id, \n",
    "        hf_token=actual_hf_token\n",
    "    )\n",
    "    print(f\"Fine-tuning script completed. Local model output path: {local_model_output_path}\")\n",
    "    # The actual S3 path will be implicitly returned by @step based on /opt/ml/model content.\n",
    "    # To make it explicit for the next step to consume, we can return a dict. The key here is conventional.\n",
    "    return {\"model_s3_path_from_opt_ml_model\": local_model_output_path} \n",
    "\n",
    "# C. Evaluation Step\n",
    "@step(\n",
    "    name=\"EvaluateFineTunedClassifier\",\n",
    "    instance_type=evaluation_instance_type,\n",
    "    image_uri=hf_pytorch_image_uri, # Reuse image\n",
    "    keep_alive_period_in_seconds=600,\n",
    ")\n",
    "def pipeline_evaluate_step(\n",
    "    finetune_output: dict, # Output from finetune_step, contains S3 path for /opt/ml/model\n",
    "    processed_data_s3_paths: dict, \n",
    "    mlflow_arn_tracking: str,\n",
    "    mlflow_exp_name: str,\n",
    "    pipeline_exec_id: str,\n",
    "    hf_auth_token: str # Parameter for HF token, might be needed if model loading requires auth\n",
    "):\n",
    "    # from steps.evaluation_classifier import evaluate_model\n",
    "    \n",
    "    print(f\"Executing evaluation. Finetune output: {finetune_output}\")\n",
    "    print(f\"MLflow ARN: {mlflow_arn_tracking}, Experiment: {mlflow_exp_name}, Run: {pipeline_exec_id}\")\n",
    "    \n",
    "    # The finetune_output['model_s3_path_from_opt_ml_model'] is the S3 URI where SageMaker \n",
    "    # stored the /opt/ml/model contents from the fine-tuning step. \n",
    "    # The evaluation_classifier.py script needs to handle loading from this S3 path.\n",
    "    # Alternatively, using MLflow model URI is often more robust.\n",
    "    \n",
    "    # Construct MLflow model URI. Assumes finetune_model logged it as 'fine_tuned_classifier_model'\n",
    "    mlflow_model_uri_to_load = f\"runs:/{pipeline_exec_id}/fine_tuned_classifier_model\"\n",
    "    print(f\"Attempting to load model for evaluation from MLflow URI: {mlflow_model_uri_to_load}\")\n",
    "    \n",
    "    # The poc_categories.json is expected to be logged by preprocess_data alongside train/val/test files.\n",
    "    # Its S3 path needs to be constructed based on one of the dataset paths.\n",
    "    # Assuming 'poc_categories.json' is at the same S3 prefix as 'train_dataset.jsonl'.\n",
    "    base_s3_dir_for_data = os.path.dirname(processed_data_s3_paths['train'])\n",
    "    poc_categories_s3 = os.path.join(base_s3_dir_for_data, \"poc_categories.json\")\n",
    "    print(f\"Path to poc_categories.json for evaluation: {poc_categories_s3}\")\n",
    "\n",
    "    eval_results = evaluate_model(\n",
    "        model_s3_path_or_mlflow_uri=mlflow_model_uri_to_load, \n",
    "        test_data_s3_path=processed_data_s3_paths['test'],\n",
    "        poc_categories_s3_path=poc_categories_s3,\n",
    "        mlflow_arn=mlflow_arn_tracking,\n",
    "        experiment_name=mlflow_exp_name,\n",
    "        run_id=pipeline_exec_id \n",
    "    )\n",
    "    print(f\"Evaluation results: {eval_results}\")\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Construct the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_step_outputs = pipeline_preprocess_step(\n",
    "    output_s3_bucket_name=default_bucket, \n",
    "    num_samples=param_num_samples_per_category,\n",
    "    langs=param_languages,\n",
    "    mlflow_arn_tracking=mlflow_tracking_server_arn, \n",
    "    mlflow_exp_name=mlflow_experiment_name, \n",
    "    pipeline_exec_id=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    ")\n",
    "\n",
    "finetune_step_outputs = pipeline_finetune_step(\n",
    "    processed_data_s3_paths=preprocess_step_outputs, \n",
    "    model_identifier=model_id, \n",
    "    epochs_ft=param_finetune_epochs,\n",
    "    lora_r_val=param_lora_r,\n",
    "    lora_alpha_val=param_lora_alpha,\n",
    "    lr_val=param_learning_rate,\n",
    "    mlflow_arn_tracking=mlflow_tracking_server_arn,\n",
    "    mlflow_exp_name=mlflow_experiment_name,\n",
    "    pipeline_exec_id=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "    hf_auth_token=param_hf_token # Pass HF token parameter\n",
    ")\n",
    "\n",
    "evaluate_step_outputs = pipeline_evaluate_step(\n",
    "    finetune_output=finetune_step_outputs, \n",
    "    processed_data_s3_paths=preprocess_step_outputs,\n",
    "    mlflow_arn_tracking=mlflow_tracking_server_arn,\n",
    "    mlflow_exp_name=mlflow_experiment_name,\n",
    "    pipeline_exec_id=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "    hf_auth_token=param_hf_token # Pass HF token parameter, eval might need for tokenizer/model if not fully self-contained\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        param_num_samples_per_category,\n",
    "        param_languages,\n",
    "        param_finetune_epochs,\n",
    "        param_lora_r,\n",
    "        param_lora_alpha,\n",
    "        param_learning_rate,\n",
    "        param_hf_token,\n",
    "    ],\n",
    "    steps=[evaluate_step_outputs], # Only need to pass the last step if they are chained correctly\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Upsert and Execute Pipeline\n",
    "\n",
    "**Important:** Before executing, ensure your MLflow Tracking Server ARN is correctly set. If it's still a placeholder, the pipeline will attempt to run but MLflow logging will fail or target a non-existent server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"your-region\" in mlflow_tracking_server_arn:\n",
    "    print(\"ERROR: MLflow Tracking Server ARN is still a placeholder. Pipeline execution will likely fail at MLflow logging steps.\")\n",
    "    print(\"Please update the 'mlflow_tracking_server_arn' variable in cell [3] before proceeding.\")\n",
    "else:\n",
    "    print(\"Upserting the pipeline...\")\n",
    "    pipeline.upsert(role_arn=role)\n",
    "    print(f\"Pipeline '{pipeline_name}' upserted.\")\n",
    "\n",
    "    print(\"Starting pipeline execution with default parameters...\")\n",
    "    # You can override default parameter values here if needed:\n",
    "    # execution = pipeline.start(\n",
    "    #     parameters={\n",
    "    #         \"NumSamplesPerCategory\": 100, \n",
    "    #         \"FineTuneEpochs\": 2,\n",
    "    #         \"Languages\": \"en\",\n",
    "    #         \"HuggingFaceToken\": \"your_actual_hf_token_if_needed_and_not_using_env_vars_or_secrets\"\n",
    "    #     }\n",
    "    # )\n",
    "    execution = pipeline.start()\n",
    "    \n",
    "    print(f\"Pipeline execution started with ARN: {execution.arn}\")\n",
    "    execution.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can monitor the pipeline execution in the SageMaker console.\n",
    "\n",
    "To run with different parameters (e.g., for a second experiment):\n",
    "```python\n",
    "# execution2 = pipeline.start(\n",
    "# parameters={\n",
    "# \"NumSamplesPerCategory\": 150,\n",
    "# \"FineTuneEpochs\": 1,\n",
    "# \"LoraR\": 16,\n",
    "# \"LoraAlpha\": 32,\n",
    "# \"Languages\": \"en,fr\",\n",
    "# # \"HuggingFaceToken\": \"your_hf_token_if_needed_for_this_run\" \n",
    "# }\n",
    "# )\n",
    "# print(f\"Second pipeline execution started with ARN: {execution2.arn}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Clean up (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To delete the pipeline definition from SageMaker after you're done:\n",
    "# try:\n",
    "#     pipeline.delete()\n",
    "#     print(f\"Pipeline '{pipeline_name}' deleted.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error deleting pipeline: {e}\")\n",
    "\n",
    "# To delete specific pipeline executions, you can use boto3:\n",
    "# sm_client = boto3.client(\"sagemaker\")\n",
    "# if 'execution' in locals() and execution:\n",
    "# try:\n",
    "#     sm_client.delete_pipeline(PipelineName=execution.arn.split('/')[-1]) # This is incorrect, use PipelineName\n",
    "#     # To stop and then delete an execution:\n",
    "#     # sm_client.stop_pipeline_execution(PipelineExecutionArn=execution.arn)\n",
    "#     # print(f\"Pipeline execution {execution.arn} stopped (if running).\")\n",
    "#     # # Deletion of execution records is usually managed by SageMaker or done via UI/CLI for specific executions.\n",
    "# except Exception as e:\n",
    "# print(f\"Error managing pipeline execution {execution.arn}: {e}\")\n",
    "pass # Placeholder to avoid empty cell issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn_aws_sagemaker_us-east-1_000000000000_kernel_data-science-3-10-py310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "sagemaker": {
   "image_uri": "arn:aws:sagemaker:us-east-1:000000000000:image/sagemaker-data-science-310-v1",
   "instance_type": "ml.t3.medium"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
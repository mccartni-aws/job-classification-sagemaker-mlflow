{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning and Evaluating LLMs with SageMaker Pipelines and MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker==2.225.0  datasets==2.18.0 transformers==4.40.0 mlflow==2.13.2 sagemaker-mlflow==0.1.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries and Setting Up Environment**\n",
    "\n",
    "This part imports all necessary Python modules. It includes SageMaker-specific imports for pipeline creation and execution, as well as user-defined functions for the pipeline steps like finetune_llama7b_hf and preprocess_llama3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 20:58:55.503015: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO:datasets:PyTorch version 2.7.0+cu118 available.\n",
      "INFO:datasets:TensorFlow version 2.12.1 available.\n",
      "INFO:datasets:JAX version 0.4.20 available.\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.function_step import step\n",
    "\n",
    "# from steps.finetune_llama8b_hf import finetune_llama8b\n",
    "# from steps.preprocess_llama3 import preprocess\n",
    "# from steps.evaluation_mlflow import evaluation\n",
    "from steps.finetune_llama3_classifier import launch_hf_training_job\n",
    "from steps.evaluation_classifier import evaluate_model\n",
    "from steps.preprocess_job_descriptions import preprocess_job_data\n",
    "\n",
    "from steps.utils import create_training_job_name\n",
    "import os\n",
    "\n",
    "# os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SageMaker Session and IAM Role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_execution_role()`: Retrieves the IAM role that SageMaker will use to access AWS resources. This role needs appropriate permissions for tasks like accessing S3 buckets and creating SageMaker resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::174671970284:role/service-role/AmazonSageMaker-ExecutionRole-20240216T153805\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    print(role)\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Configuration**\n",
    "\n",
    "The train_config dictionary is comprehensive, including:\n",
    "\n",
    "Experiment naming for tracking purposes\n",
    "Model specifications (ID, version, name)\n",
    "Infrastructure details (instance types and counts for fine-tuning and deployment)\n",
    "Training hyperparameters (epochs, batch size)\n",
    "\n",
    "This configuration allows for easy adjustment of the training process without changing the core pipeline code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = {\n",
    "    \"experiment_name\": \"all_target_modules_1K\",\n",
    "    \"model_id\": \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"model_version\": \"3.0.2\",\n",
    "    \"model_name\": \"llama-3-8b\",\n",
    "    \"endpoint_name\": \"llama-3-8b\",\n",
    "    \"finetune_instance_type\": \"ml.g5.12xlarge\",\n",
    "    \"finetune_num_instances\": 1,\n",
    "    \"instance_type\": \"ml.g5.12xlarge\",\n",
    "    \"num_instances\": 1,\n",
    "    \"epoch\": 1,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LoRA Parameters**\n",
    "\n",
    "Low-Rank Adaptation (LoRA) is an efficient fine-tuning technique for large language models. The parameters here (lora_r, lora_alpha, lora_dropout) control the behavior of LoRA during fine-tuning, affecting the trade-off between model performance and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_params = {\"lora_r\": 8, \"lora_alpha\": 16, \"lora_dropout\": 0.05}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. MLflow Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow integration is crucial for experiment tracking and management.\n",
    "\n",
    "mlflow_arn: The ARN for the MLflow tracking server. You can get this ARN from SageMaker Studio UI. This allows the pipeline to log metrics, parameters, and artifacts to a central location.\n",
    "\n",
    "experiment_name: give appropriate name for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow_arn = \"<MLflow_tracking_server_ARN>\"  # fill MLflow tracking server ARN\n",
    "# experiment_name = \"sm-pipelines-finetuning-eval\"\n",
    "mlflow_arn = \"arn:aws:sagemaker:us-east-1:174671970284:mlflow-tracking-server/mlflow-d-8mkvrvo3fobb-27-10-47-37\" # <--- REPLACE THIS\n",
    "experiment_name = \"JobDescriptionClassification-Llama3-FineTuning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Dataset Configuration\n",
    "\n",
    "For the purpose of fine tuning and evaluation we are going too use `HuggingFaceH4/no_robots` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = \"JobDescClassification-Llama3-Pipeline-V5\" \n",
    "base_job_prefix = \"job-desc-classify\"\n",
    "\n",
    "processed_data_s3_prefix = f\"{base_job_prefix}/processed_data/v3\"\n",
    "\n",
    "default_raw_data_s3_uri = \"s3://sagemaker-us-east-1-174671970284/raw_job_data/poc_multilingual_set_20250604_214156/raw_jds_translated_v2.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Pipeline Steps\n",
    "\n",
    "This section defines the core components of the SageMaker pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterString\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = ParameterString(name=\"lora_config\", default_value=json.dumps(lora_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing Step**\n",
    "\n",
    "This step handles data preparation. We are going to prepare data for training and evaluation. We will log this data in MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 20:59:01,376 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-174671970284/JobDescPreprocessPipeline/PreprocessJobDescriptions/2025-06-05-20-59-01-160/function\n",
      "2025-06-05 20:59:01,426 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-174671970284/JobDescPreprocessPipeline/PreprocessJobDescriptions/2025-06-05-20-59-01-160/arguments\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-06-05 20:59:01,853 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-174671970284/JobDescPreprocessPipeline/PreprocessJobDescriptions/2025-06-05-20-59-01-853/function\n",
      "2025-06-05 20:59:01,913 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-174671970284/JobDescPreprocessPipeline/PreprocessJobDescriptions/2025-06-05-20-59-01-853/arguments\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:174671970284:pipeline/JobDescPreprocessPipeline',\n",
       " 'ResponseMetadata': {'RequestId': 'c1f02539-6866-4ee3-bae6-46cfa82b304c',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'c1f02539-6866-4ee3-bae6-46cfa82b304c',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '93',\n",
       "   'date': 'Thu, 05 Jun 2025 20:59:02 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.function_step import step\n",
    "from sagemaker.workflow.parameters import ParameterString # If needed for other params\n",
    "\n",
    "# Assuming preprocess_job_descriptions.py is in a 'steps' directory\n",
    "from steps.preprocess_job_descriptions import preprocess_job_data\n",
    "\n",
    "# Define parameters for the preprocess step\n",
    "s3_bucket_name = sagemaker.Session().default_bucket() # or your specific bucket\n",
    "\n",
    "output_s3_prefix_jobs = \"processed_data/my_job_class_experiment\"\n",
    "\n",
    "# Create the preprocessing step using the imported function\n",
    "preprocess_jobs_step = step(\n",
    "    preprocess_job_data,\n",
    "    # instance_type=\"ml.g5.12xlarge\",\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    name=\"PreprocessJobDescriptions\" # SageMaker step name\n",
    ")(\n",
    "    raw_dataset_identifier=default_raw_data_s3_uri,\n",
    "    s3_output_bucket=s3_bucket_name,\n",
    "    s3_output_prefix=output_s3_prefix_jobs,\n",
    "    job_desc_column=\"description\", # Example: if your column is named 'description'\n",
    "    category_column=\"job_category\", # Example: if your column is named 'job_category'\n",
    "    max_samples_per_split=1000, # Optional: for faster testing\n",
    "    mlflow_arn=mlflow_arn,       # Your MLflow tracking server ARN\n",
    "    experiment_name=experiment_name, # Your MLflow experiment name for preprocessing\n",
    "    run_name=ExecutionVariables.PIPELINE_EXECUTION_ID, # Links MLflow run to pipeline execution\n",
    ")\n",
    "\n",
    "# Example: Define a pipeline with just this step\n",
    "pipeline_name_jobs = \"JobDescPreprocessPipeline\"\n",
    "job_pipeline = Pipeline(\n",
    "    name=pipeline_name_jobs,\n",
    "    steps=[preprocess_jobs_step],\n",
    "    # parameters=[...] # if you have pipeline-level parameters\n",
    "    sagemaker_session=sess,\n",
    "    # instance_type=\"ml.g5.12xlarge\",\n",
    "    # volume_size=100,\n",
    ")\n",
    "\n",
    "# Upsert and run the pipeline\n",
    "job_pipeline.upsert(role_arn=role)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'081325390199.dkr.ecr.us-east-1.amazonaws.com/sagemaker-base-python-310:1.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.image_uris.get_base_python_image_uri('us-east-1', py_version='310')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_execution = job_pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DelayedReturn' object has no attribute 'properties'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/sagemaker-user/job-classification-sagemaker-mlflow/llm_fine_tuning_experiments_mlflow(1).ipynb Cell 27\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://gom8fjg3ypjgkmk.studio.us-east-1.sagemaker.aws/home/sagemaker-user/job-classification-sagemaker-mlflow/llm_fine_tuning_experiments_mlflow%281%29.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# You can then access the outputs:\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://gom8fjg3ypjgkmk.studio.us-east-1.sagemaker.aws/home/sagemaker-user/job-classification-sagemaker-mlflow/llm_fine_tuning_experiments_mlflow%281%29.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m training_data_path \u001b[39m=\u001b[39m preprocess_jobs_step\u001b[39m.\u001b[39;49mproperties\u001b[39m.\u001b[39mOutputs[\u001b[39m'\u001b[39m\u001b[39mtrain_data_s3_path\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://gom8fjg3ypjgkmk.studio.us-east-1.sagemaker.aws/home/sagemaker-user/job-classification-sagemaker-mlflow/llm_fine_tuning_experiments_mlflow%281%29.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m validation_data_path \u001b[39m=\u001b[39m preprocess_jobs_step\u001b[39m.\u001b[39mproperties\u001b[39m.\u001b[39mOutputs[\u001b[39m'\u001b[39m\u001b[39mvalidation_data_s3_path\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://gom8fjg3ypjgkmk.studio.us-east-1.sagemaker.aws/home/sagemaker-user/job-classification-sagemaker-mlflow/llm_fine_tuning_experiments_mlflow%281%29.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m mlflow_run_id_preprocess \u001b[39m=\u001b[39m preprocess_jobs_step\u001b[39m.\u001b[39mproperties\u001b[39m.\u001b[39mOutputs[\u001b[39m'\u001b[39m\u001b[39mmlflow_run_id\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DelayedReturn' object has no attribute 'properties'"
     ]
    }
   ],
   "source": [
    "# You can then access the outputs:\n",
    "training_data_path = preprocess_jobs_step.properties.Outputs['train_data_s3_path']\n",
    "validation_data_path = preprocess_jobs_step.properties.Outputs['validation_data_s3_path']\n",
    "mlflow_run_id_preprocess = preprocess_jobs_step.properties.Outputs['mlflow_run_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_name = \"fmops-training-evaulation-pipeline-mlflow\"\n",
    "\n",
    "# default_bucket = sagemaker.Session().default_bucket()\n",
    "# main_data_path = f\"s3://{default_bucket}\"\n",
    "# evaluation_data_path = (\n",
    "#     main_data_path\n",
    "#     + \"/datasets/hf_no_robots/evaluation/automatic_small/dataset_evaluation_small.jsonl\"\n",
    "# )\n",
    "# output_data_path = main_data_path + \"/datasets/hf_no_robots/output_\" + pipeline_name\n",
    "\n",
    "# # You can add your own evaluation dataset code into this step\n",
    "# preprocess_step_ret = step(preprocess, name=\"preprocess\")(\n",
    "#     default_bucket,\n",
    "#     dataset_name,\n",
    "#     train_sample=100,\n",
    "#     eval_sample=100,\n",
    "#     mlflow_arn=mlflow_arn,\n",
    "#     experiment_name=experiment_name,\n",
    "#     run_name=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "# )\n",
    "\n",
    "# print(\"The pipeline name is \" + pipeline_name)\n",
    "# # Mark the name of this bucket for reviewing the artifacts generated by this pipeline at the end of the execution\n",
    "# print(\"Output S3 bucket: \" + output_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine-tuning Step**\n",
    "\n",
    "This is where the actual model adaptation occurs. The step takes the preprocessed data and applies it to fine-tune the base LLM (in this case, a Llama model). It incorporates the LoRA technique for efficient adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_ret = step(finetune_llama7b, name=\"finetune_llama8b_instruction\")(\n",
    "    preprocess_step_ret,\n",
    "    train_config,\n",
    "    lora_config,\n",
    "    role,\n",
    "    mlflow_arn,\n",
    "    experiment_name,\n",
    "    ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation Step**\n",
    "\n",
    "After fine-tuning, this step assesses the model's performance. It uses built-in evaluation function in MLflow to evaluate metrices like toxicity, exact_match etc:\n",
    "\n",
    "It will then log the results in MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_finetuned_llama7b_instruction_mlflow = step(\n",
    "    evaluation,\n",
    "    name=\"evaluate_finetuned_llama8b_instr\",\n",
    "    # keep_alive_period_in_seconds=1200,\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    volume_size=100,\n",
    ")(train_config, preprocess_step_ret, finetune_ret, mlflow_arn, experiment_name, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Pipeline Creation and Execution\n",
    "\n",
    "This final section brings all the components together into an executable pipeline.\n",
    "\n",
    "**Creating the Pipeline**\n",
    "\n",
    "The pipeline object is created with all defined steps. The lora_config is passed as a parameter, allowing for easy modification of LoRA settings between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    steps=[evaluate_finetuned_llama7b_instruction_mlflow],\n",
    "    parameters=[lora_config],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upserting the Pipeline**\n",
    "\n",
    "This step either creates a new pipeline in SageMaker or updates an existing one with the same name. It's a key part of the MLOps process, allowing for iterative refinement of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Starting the Pipeline Execution**\n",
    "\n",
    "This command kicks off the actual execution of the pipeline in SageMaker. From this point, SageMaker will orchestrate the execution of each step, managing resources and data flow between steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution1 = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets run another experiment with different LORA configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_params_2 = {\"lora_r\": 32, \"lora_alpha\": 64, \"lora_dropout\": 0.05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution2 = pipeline.start(\n",
    "    parameters={\n",
    "        \"lora_config\": json.dumps(lora_params_2),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "response = sagemaker_client.delete_pipeline(\n",
    "    PipelineName=pipeline_name,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

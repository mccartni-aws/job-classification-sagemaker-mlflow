{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Nick) Fine-Tuning and Evaluating LLMs with SageMaker Pipelines and MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker==2.225.0  datasets==2.18.0 transformers==4.40.0 mlflow==2.13.2 sagemaker-mlflow==0.1.0 protobuf==3.20.3 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries and Setting Up Environment**\n",
    "\n",
    "This part imports all necessary Python modules. It includes SageMaker-specific imports for pipeline creation and execution, as well as user-defined functions for the pipeline steps like finetune_llama7b_hf and preprocess_llama3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.9 | packaged by conda-forge | (main, Feb 14 2025, 08:00:06) [GCC 13.3.0]\n",
      "Python version info: sys.version_info(major=3, minor=12, micro=9, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python version info: {sys.version_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.5.1 available.\n",
      "INFO:datasets:TensorFlow version 2.18.0 available.\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.function_step import step\n",
    "\n",
    "# from steps.finetune_llama8b_hf import finetune_llama8b\n",
    "# from steps.preprocess_llama3 import preprocess\n",
    "# from steps.evaluation_mlflow import evaluation\n",
    "from steps.finetune_llama3_classifier import finetune_classifier_hf\n",
    "# from steps.evaluation_classifier import evaluate_model\n",
    "from steps.preprocess_job_descriptions import preprocess_job_data\n",
    "\n",
    "from steps.utils import create_training_job_name\n",
    "import os\n",
    "\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'081325390199.dkr.ecr.us-east-1.amazonaws.com/sagemaker-base-python-312:1.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.image_uris.get_base_python_image_uri('us-east-1', py_version='312')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SageMaker Session and IAM Role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_execution_role()`: Retrieves the IAM role that SageMaker will use to access AWS resources. This role needs appropriate permissions for tasks like accessing S3 buckets and creating SageMaker resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Fetched defaults config from location: /home/sagemaker-user/job-classification-sagemaker-mlflow\n",
      "arn:aws:iam::174671970284:role/service-role/AmazonSageMaker-ExecutionRole-20240216T153805\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    print(role)\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Configuration**\n",
    "\n",
    "The train_config dictionary is comprehensive, including:\n",
    "\n",
    "Experiment naming for tracking purposes\n",
    "Model specifications (ID, version, name)\n",
    "Infrastructure details (instance types and counts for fine-tuning and deployment)\n",
    "Training hyperparameters (epochs, batch size)\n",
    "\n",
    "This configuration allows for easy adjustment of the training process without changing the core pipeline code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = {\n",
    "    \"experiment_name\": \"progressive_metrics_test\",\n",
    "    \"model_id\": \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"model_version\": \"3.0.2\",\n",
    "    \"model_name\": \"llama-3-8b\",\n",
    "    \"endpoint_name\": \"llama-3-8b\",\n",
    "    \"finetune_instance_type\": \"ml.g5.24xlarge\",\n",
    "    \"source_directory\": \"scripts/python\",  # Make sure this contains your utils folder\n",
    "    \"entry_point_script\": \"finetune_entrypoint.py\",\n",
    "    \"merge_weights\": True,\n",
    "    \"finetune_num_instances\": 1,\n",
    "    \"instance_type\": \"ml.g5.12xlarge\",\n",
    "    \"num_instances\": 1,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"epoch\": 2,  # 2 epochs for better curves\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"max_seq_length\": 512,\n",
    "    \n",
    "    # Progressive logging settings\n",
    "    \"limit_train_samples\": 500,   # More samples for longer training\n",
    "    \"limit_eval_samples\": 50,     # More eval samples\n",
    "    \"logging_steps\": 3,           # Log every 3 steps\n",
    "    \"eval_steps\": 6,              # Evaluate every 6 steps\n",
    "    \n",
    "    \"bf16\": True,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"hf_token\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LoRA Parameters**\n",
    "\n",
    "Low-Rank Adaptation (LoRA) is an efficient fine-tuning technique for large language models. The parameters here (lora_r, lora_alpha, lora_dropout) control the behavior of LoRA during fine-tuning, affecting the trade-off between model performance and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_params = {\"lora_r\": 8, \"lora_alpha\": 16, \"lora_dropout\": 0.05, \"merge_weights\": True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. MLflow Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow integration is crucial for experiment tracking and management.\n",
    "\n",
    "mlflow_arn: The ARN for the MLflow tracking server. You can get this ARN from SageMaker Studio UI. This allows the pipeline to log metrics, parameters, and artifacts to a central location.\n",
    "\n",
    "experiment_name: give appropriate name for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow_arn = \"<MLflow_tracking_server_ARN>\"  # fill MLflow tracking server ARN\n",
    "# experiment_name = \"sm-pipelines-finetuning-eval\"\n",
    "mlflow_arn = \"arn:aws:sagemaker:us-east-1:174671970284:mlflow-tracking-server/mlflow-d-8mkvrvo3fobb-27-10-47-37\" # <--- REPLACE THIS\n",
    "experiment_name = \"JobDescriptionClassification-Llama3-FineTuning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Dataset Configuration\n",
    "\n",
    "For the purpose of fine tuning and evaluation we are going too use `HuggingFaceH4/no_robots` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = \"JobDescClassification-Llama3-Pipeline-V5\" \n",
    "base_job_prefix = \"job-desc-classify\"\n",
    "\n",
    "processed_data_s3_prefix = f\"{base_job_prefix}/processed_data/v3\"\n",
    "\n",
    "# default_raw_data_s3_uri = \"s3://sagemaker-us-east-1-174671970284/raw_job_data/poc_multilingual_set_20250604_214156/raw_jds_translated_v2.jsonl\"\n",
    "default_raw_data_s3_uri = \"s3://sagemaker-us-east-1-174671970284/raw_job_description_data/v2_translated/raw_jds_translated_v2.jsonl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# STANDALONE EVALUATION - Use this to evaluate an existing fine-tuned model\n",
    "\n",
    "# Step 1: Define your model and data paths\n",
    "# Replace these with your actual S3 paths\n",
    "model_s3_path = \"s3://sagemaker-us-east-1-174671970284/huggingface-qlora-1-8-2025-06-09-13-13--2025-06-09-13-13-39-015/output/\"  # From your fine-tuning job\n",
    "test_data_s3_path = \"s3://sagemaker-us-east-1-174671970284/processed_data/my_job_class_experiment/test/test_dataset.jsonl\"  # Your test data\n",
    "poc_categories_s3_path = \"s3://sagemaker-us-east-1-174671970284/processed_data/my_job_class_experiment/poc_categories.json\"  # Your categories\n",
    "\n",
    "# Step 2: Import and create evaluation step\n",
    "from steps.evaluation_classifier import evaluate_model\n",
    "from sagemaker.workflow.function_step import step\n",
    "\n",
    "# Create standalone evaluation step\n",
    "standalone_evaluate_step = step(\n",
    "    evaluate_model,\n",
    "    instance_type=\"ml.g5.24xlarge\",  # You can use smaller instance\n",
    "    name=\"StandaloneEvaluation\"\n",
    ")(\n",
    "    model_s3_path_or_mlflow_uri=model_s3_path,\n",
    "    test_data_s3_path=test_data_s3_path,\n",
    "    poc_categories_s3_path=poc_categories_s3_path,\n",
    "    batch_size=4,  # Smaller batch for memory efficiency\n",
    "    mlflow_arn=mlflow_arn,\n",
    "    experiment_name=experiment_name,\n",
    "    run_id=None  # Will create new MLflow run\n",
    ")\n",
    "\n",
    "# Step 3: Create and run standalone pipeline\n",
    "standalone_pipeline_name = \"StandaloneEvaluationPipeline\"\n",
    "standalone_pipeline = Pipeline(\n",
    "    name=standalone_pipeline_name,\n",
    "    steps=[standalone_evaluate_step],\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "standalone_pipeline.upsert(role_arn=role)\n",
    "standalone_execution = standalone_pipeline.start()\n",
    "\n",
    "print(f\"Standalone evaluation started: {standalone_execution.describe()}\")\n",
    "\n",
    "# Wait for completion and get results\n",
    "# standalone_execution.wait()\n",
    "# print(\"Standalone evaluation completed!\")\n",
    "\n",
    "# Alternative: Run evaluation locally (if you have the model downloaded)\n",
    "# This would run evaluation on your notebook instance instead of SageMaker\n",
    "\"\"\"\n",
    "# LOCAL EVALUATION EXAMPLE (uncomment to use)\n",
    "import sys\n",
    "sys.path.append('steps')  # Add steps to path\n",
    "from evaluate_classifier import evaluate_model\n",
    "\n",
    "# Run evaluation locally\n",
    "local_results = evaluate_model(\n",
    "    model_s3_path_or_mlflow_uri=\"s3://your-model-path/\",\n",
    "    test_data_s3_path=\"s3://your-test-data-path/test_dataset.jsonl\",\n",
    "    poc_categories_s3_path=\"s3://your-categories-path/poc_categories.json\",\n",
    "    batch_size=2,  # Smaller batch for local execution\n",
    "    mlflow_arn=mlflow_arn,\n",
    "    experiment_name=experiment_name,\n",
    "    run_id=None\n",
    ")\n",
    "\n",
    "print(f\"Local evaluation results: {local_results}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Pipeline Steps\n",
    "\n",
    "This section defines the core components of the SageMaker pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterString\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = ParameterString(name=\"lora_config\", default_value=json.dumps(lora_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing Step**\n",
    "\n",
    "This step handles data preparation. We are going to prepare data for training and evaluation. We will log this data in MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pipeline name is JobDescClassification-Llama3-Pipeline-V5\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.function_step import step\n",
    "from sagemaker.workflow.parameters import ParameterString # If needed for other params\n",
    "\n",
    "# Assuming preprocess_job_descriptions.py is in a 'steps' directory\n",
    "from steps.preprocess_job_descriptions import preprocess_job_data\n",
    "\n",
    "# Define parameters for the preprocess step\n",
    "s3_bucket_name = sagemaker.Session().default_bucket() # or your specific bucket\n",
    "\n",
    "output_s3_prefix_jobs = \"processed_data/my_job_class_experiment\"\n",
    "\n",
    "# Create the preprocessing step using the imported function\n",
    "preprocess_jobs_step = step(\n",
    "    preprocess_job_data,\n",
    "    # instance_type=\"ml.g5.12xlarge\",\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    name=\"PreprocessJobDescriptions\" # SageMaker step name\n",
    ")(\n",
    "    raw_dataset_identifier=default_raw_data_s3_uri,\n",
    "    s3_output_bucket=s3_bucket_name,\n",
    "    s3_output_prefix=output_s3_prefix_jobs,\n",
    "    job_desc_column=\"job_description_text\", # Example: if your column is named 'description'\n",
    "    category_column=\"category_label\", # Example: if your column is named 'job_category'\n",
    "    max_samples_per_split=1000, # Optional: for faster testing\n",
    "    mlflow_arn=mlflow_arn,       # Your MLflow tracking server ARN\n",
    "    experiment_name=experiment_name, # Your MLflow experiment name for preprocessing\n",
    "    run_name=ExecutionVariables.PIPELINE_EXECUTION_ID, # Links MLflow run to pipeline execution\n",
    ")\n",
    "\n",
    "print(\"The pipeline name is \" + pipeline_name)\n",
    "# Mark the name of this bucket for reviewing the artifacts generated by this pipeline at the end of the execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from steps.finetune_llama3_classifier import finetune_classifier_hf\n",
    "\n",
    "# Create the fine-tuning step\n",
    "finetune_job_classifier_step = step(\n",
    "    finetune_classifier_hf,\n",
    "    name=\"FineTuneJobClassifier\"\n",
    ")(\n",
    "    preprocess_step_output=preprocess_jobs_step, # Pass the entire output dict\n",
    "    train_config=train_config,\n",
    "    lora_config=lora_config,\n",
    "    role=role,\n",
    "    mlflow_arn=mlflow_arn,\n",
    "    experiment_name=experiment_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess and FT PLINE**|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.Dependencies\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.IncludeLocalWorkDir\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.CustomFileFilter.IgnoreNamePatterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 17:56:41,108 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-174671970284/JobDescPreprocessFineTunePipeline/PreprocessJobDescriptions/2025-06-09-17-56-39-566/function\n",
      "2025-06-09 17:56:41,199 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-174671970284/JobDescPreprocessFineTunePipeline/PreprocessJobDescriptions/2025-06-09-17-56-39-566/arguments\n",
      "2025-06-09 17:56:41,425 sagemaker.remote_function INFO     Copied dependencies file at './requirements.txt' to '/tmp/tmp742kbk89/requirements.txt'\n",
      "2025-06-09 17:56:41,454 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-174671970284/JobDescPreprocessFineTunePipeline/PreprocessJobDescriptions/2025-06-09-17-56-39-566/pre_exec_script_and_dependencies'\n",
      "2025-06-09 17:56:41,464 sagemaker.remote_function INFO     Copied user workspace to '/tmp/tmpi8mz1uog/temp_workspace/sagemaker_remote_function_workspace'\n",
      "2025-06-09 17:56:41,484 sagemaker.remote_function INFO     Successfully created workdir archive at '/tmp/tmpi8mz1uog/workspace.zip'\n",
      "2025-06-09 17:56:41,615 sagemaker.remote_function INFO     Successfully uploaded workdir to 's3://sagemaker-us-east-1-174671970284/JobDescPreprocessFineTunePipeline/sm_rf_user_ws/2025-06-09-17-56-39-566/workspace.zip'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.Dependencies\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.IncludeLocalWorkDir\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.CustomFileFilter.IgnoreNamePatterns\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.InstanceType\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 17:56:43,171 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-174671970284/JobDescPreprocessFineTunePipeline/FineTuneJobClassifier/2025-06-09-17-56-39-566/function\n",
      "2025-06-09 17:56:43,223 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-174671970284/JobDescPreprocessFineTunePipeline/FineTuneJobClassifier/2025-06-09-17-56-39-566/arguments\n",
      "2025-06-09 17:56:43,296 sagemaker.remote_function INFO     Copied dependencies file at './requirements.txt' to '/tmp/tmpuwk4yui2/requirements.txt'\n",
      "2025-06-09 17:56:43,326 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-174671970284/JobDescPreprocessFineTunePipeline/FineTuneJobClassifier/2025-06-09-17-56-39-566/pre_exec_script_and_dependencies'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-06-09 17:56:43,771 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-174671970284/JobDescPreprocessFineTunePipeline/PreprocessJobDescriptions/2025-06-09-17-56-43-771/function\n",
      "2025-06-09 17:56:43,831 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-174671970284/JobDescPreprocessFineTunePipeline/PreprocessJobDescriptions/2025-06-09-17-56-43-771/arguments\n",
      "2025-06-09 17:56:44,023 sagemaker.remote_function INFO     Copied dependencies file at './requirements.txt' to '/tmp/tmp8v3j54lc/requirements.txt'\n",
      "2025-06-09 17:56:44,054 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-174671970284/JobDescPreprocessFineTunePipeline/PreprocessJobDescriptions/2025-06-09-17-56-43-771/pre_exec_script_and_dependencies'\n",
      "2025-06-09 17:56:44,064 sagemaker.remote_function INFO     Copied user workspace to '/tmp/tmp10td_1au/temp_workspace/sagemaker_remote_function_workspace'\n",
      "2025-06-09 17:56:44,084 sagemaker.remote_function INFO     Successfully created workdir archive at '/tmp/tmp10td_1au/workspace.zip'\n",
      "2025-06-09 17:56:44,233 sagemaker.remote_function INFO     Successfully uploaded workdir to 's3://sagemaker-us-east-1-174671970284/JobDescPreprocessFineTunePipeline/sm_rf_user_ws/2025-06-09-17-56-43-771/workspace.zip'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-06-09 17:56:44,237 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-174671970284/JobDescPreprocessFineTunePipeline/FineTuneJobClassifier/2025-06-09-17-56-43-771/function\n",
      "2025-06-09 17:56:44,303 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-174671970284/JobDescPreprocessFineTunePipeline/FineTuneJobClassifier/2025-06-09-17-56-43-771/arguments\n",
      "2025-06-09 17:56:44,393 sagemaker.remote_function INFO     Copied dependencies file at './requirements.txt' to '/tmp/tmpwp7u5xu5/requirements.txt'\n",
      "2025-06-09 17:56:44,427 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-174671970284/JobDescPreprocessFineTunePipeline/FineTuneJobClassifier/2025-06-09-17-56-43-771/pre_exec_script_and_dependencies'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:174671970284:pipeline/JobDescPreprocessFineTunePipeline',\n",
       " 'ResponseMetadata': {'RequestId': 'c2d03cf7-e263-4fda-821a-06be3994c6f7',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'c2d03cf7-e263-4fda-821a-06be3994c6f7',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '101',\n",
       "   'date': 'Mon, 09 Jun 2025 17:56:44 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Define a pipeline with just this step\n",
    "pipeline_name_jobs = \"JobDescPreprocessFineTunePipeline\"\n",
    "\n",
    "job_pipeline = Pipeline(\n",
    "    name=pipeline_name_jobs,\n",
    "    steps=[preprocess_jobs_step, finetune_job_classifier_step],\n",
    "    parameters=[lora_config], # if you have pipeline-level parameters\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "# Upsert and run the pipeline\n",
    "job_pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline execution started: {'PipelineArn': 'arn:aws:sagemaker:us-east-1:174671970284:pipeline/JobDescPreprocessFineTunePipeline', 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:174671970284:pipeline/JobDescPreprocessFineTunePipeline/execution/wei8jkm553r4', 'PipelineExecutionDisplayName': 'execution-1749491806927', 'PipelineExecutionStatus': 'Executing', 'CreationTime': datetime.datetime(2025, 6, 9, 17, 56, 46, 829000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2025, 6, 9, 17, 56, 46, 829000, tzinfo=tzlocal()), 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:174671970284:user-profile/d-8mkvrvo3fobb/default-20240216t153804', 'UserProfileName': 'default-20240216t153804', 'DomainId': 'd-8mkvrvo3fobb', 'IamIdentity': {'Arn': 'arn:aws:sts::174671970284:assumed-role/AmazonSageMaker-ExecutionRole-20240216T153805/SageMaker', 'PrincipalId': 'AROASRK2CX7WPM2ML6UZA:SageMaker'}}, 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:174671970284:user-profile/d-8mkvrvo3fobb/default-20240216t153804', 'UserProfileName': 'default-20240216t153804', 'DomainId': 'd-8mkvrvo3fobb', 'IamIdentity': {'Arn': 'arn:aws:sts::174671970284:assumed-role/AmazonSageMaker-ExecutionRole-20240216T153805/SageMaker', 'PrincipalId': 'AROASRK2CX7WPM2ML6UZA:SageMaker'}}, 'ResponseMetadata': {'RequestId': '3e415d20-ba7c-4811-8fd6-5c4370ee8ef8', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '3e415d20-ba7c-4811-8fd6-5c4370ee8ef8', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1131', 'date': 'Mon, 09 Jun 2025 17:56:47 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "execution = job_pipeline.start()\n",
    "print(f\"Pipeline execution started: {execution.describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FULL PIPELINE FROM HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from steps.evaluate_classifier import evaluate_model\n",
    "\n",
    "# Create evaluation step that takes fine-tuning output\n",
    "evaluate_classifier_step = step(\n",
    "    evaluate_model,\n",
    "    instance_type=\"ml.g5.xlarge\",  # Smaller instance for evaluation\n",
    "    name=\"EvaluateJobClassifier\"\n",
    ")(\n",
    "    # Use output from fine-tuning step\n",
    "    model_s3_path_or_mlflow_uri=finetune_job_classifier_step[\"s3_model_artifacts\"],\n",
    "    # Use test data from preprocessing step  \n",
    "    test_data_s3_path=preprocess_jobs_step[\"test_data_s3_path\"],\n",
    "    # Use categories from preprocessing step\n",
    "    poc_categories_s3_path=preprocess_jobs_step[\"poc_categories_s3_path\"],\n",
    "    batch_size=4,\n",
    "    mlflow_arn=mlflow_arn,\n",
    "    experiment_name=experiment_name,\n",
    "    run_id=finetune_job_classifier_step[\"mlflow_run_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline_name_jobs = \"JobDescPreprocessFineTuneEvalPipeline\"\n",
    "job_pipeline = Pipeline(\n",
    "    name=pipeline_name_jobs,\n",
    "    steps=[\n",
    "        preprocess_jobs_step, \n",
    "        finetune_job_classifier_step,\n",
    "        evaluate_classifier_step  # NEW: Added evaluation step\n",
    "    ],\n",
    "    parameters=[lora_config],\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsert and run the pipeline\n",
    "job_pipeline.upsert(role_arn=role)\n",
    "\n",
    "print(f\"Pipeline '{pipeline_name_jobs}' created with preprocessing, fine-tuning, and evaluation steps!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Monitor Pipeline (Optional)\n",
    "execution.wait()\n",
    "print(\"Pipeline execution completed!\")\n",
    "\n",
    "# Get evaluation results\n",
    "steps_result = execution.list_steps()\n",
    "for step_info in steps_result:\n",
    "    if step_info['StepName'] == 'EvaluateJobClassifier':\n",
    "        print(f\"Evaluation step status: {step_info['StepStatus']}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_jobs_step[\"train_data_s3_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can then access the outputs:\n",
    "training_data_path = preprocess_jobs_step.properties.Outputs['train_data_s3_path']\n",
    "validation_data_path = preprocess_jobs_step.properties.Outputs['validation_data_s3_path']\n",
    "mlflow_run_id_preprocess = preprocess_jobs_step.properties.Outputs['mlflow_run_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Pipeline Creation and Execution\n",
    "\n",
    "This final section brings all the components together into an executable pipeline.\n",
    "\n",
    "**Creating the Pipeline**\n",
    "\n",
    "The pipeline object is created with all defined steps. The lora_config is passed as a parameter, allowing for easy modification of LoRA settings between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    steps=[evaluate_finetuned_llama7b_instruction_mlflow],\n",
    "    parameters=[lora_config],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upserting the Pipeline**\n",
    "\n",
    "This step either creates a new pipeline in SageMaker or updates an existing one with the same name. It's a key part of the MLOps process, allowing for iterative refinement of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Starting the Pipeline Execution**\n",
    "\n",
    "This command kicks off the actual execution of the pipeline in SageMaker. From this point, SageMaker will orchestrate the execution of each step, managing resources and data flow between steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution1 = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets run another experiment with different LORA configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "response = sagemaker_client.delete_pipeline(\n",
    "    PipelineName=pipeline_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifacts S3 path: s3://sagemaker-us-east-1-174671970284/huggingface-qlora-1-8-2025-06-09-13-13--2025-06-09-13-13-39-015/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# METHOD 1: Get model path from completed fine-tuning job\n",
    "import boto3\n",
    "\n",
    "def get_model_artifacts_from_training_job(training_job_name):\n",
    "    \"\"\"Get S3 model artifacts path from a training job name\"\"\"\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    \n",
    "    response = sm_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "    model_artifacts_s3_path = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "    \n",
    "    print(f\"Model artifacts S3 path: {model_artifacts_s3_path}\")\n",
    "    return model_artifacts_s3_path\n",
    "\n",
    "# Use this if you know your training job name\n",
    "training_job_name = \"huggingface-qlora-1-8-2025-06-09-13-13--2025-06-09-13-13-39-015\"  # Replace with actual name\n",
    "model_s3_path = get_model_artifacts_from_training_job(training_job_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: sagemaker-us-east-1-174671970284\n",
      "Prefix: huggingface-qlora-1-8-2025-06-09-13-13--2025-06-09-13-13-39-015/output/\n",
      "\n",
      "Found 1 objects:\n",
      "  huggingface-qlora-1-8-2025-06-09-13-13--2025-06-09-13-13-39-015/output/model.tar.gz\n",
      "\n",
      "WARNING: Missing required files: ['config.json', 'pytorch_model.bin', 'tokenizer.json']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add this to your notebook to check the S3 path first\n",
    "import boto3\n",
    "\n",
    "def check_s3_path(s3_path):\n",
    "    \"\"\"Check what's actually in the S3 path\"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # Parse S3 path\n",
    "    bucket_name = s3_path.replace('s3://', '').split('/')[0]\n",
    "    prefix = '/'.join(s3_path.replace('s3://', '').split('/')[1:])\n",
    "    \n",
    "    print(f\"Bucket: {bucket_name}\")\n",
    "    print(f\"Prefix: {prefix}\")\n",
    "    \n",
    "    try:\n",
    "        paginator = s3.get_paginator('list_objects_v2')\n",
    "        objects = []\n",
    "        \n",
    "        for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "            if 'Contents' in page:\n",
    "                for obj in page['Contents']:\n",
    "                    objects.append(obj['Key'])\n",
    "        \n",
    "        print(f\"\\nFound {len(objects)} objects:\")\n",
    "        for obj in objects[:10]:  # Show first 10\n",
    "            print(f\"  {obj}\")\n",
    "        \n",
    "        if len(objects) > 10:\n",
    "            print(f\"  ... and {len(objects) - 10} more\")\n",
    "        \n",
    "        # Check for required model files\n",
    "        required_files = ['config.json', 'pytorch_model.bin', 'tokenizer.json']\n",
    "        missing_files = []\n",
    "        \n",
    "        for req_file in required_files:\n",
    "            if not any(req_file in obj for obj in objects):\n",
    "                missing_files.append(req_file)\n",
    "        \n",
    "        if missing_files:\n",
    "            print(f\"\\nWARNING: Missing required files: {missing_files}\")\n",
    "        else:\n",
    "            print(f\"\\nâœ“ All required model files found!\")\n",
    "            \n",
    "        return len(objects) > 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking S3 path: {e}\")\n",
    "        return False\n",
    "\n",
    "# Replace with your actual model S3 path\n",
    "model_s3_path = \"s3://sagemaker-us-east-1-174671970284/huggingface-qlora-1-8-2025-06-09-13-13--2025-06-09-13-13-39-015/output/\"  # UPDATE THIS\n",
    "check_s3_path(model_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
